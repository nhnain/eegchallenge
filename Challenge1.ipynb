{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhnain/eegchallenge/blob/main/Challenge1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Preprocessing"
      ],
      "metadata": {
        "id": "7r4tZAQKkJFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install braindecode\n",
        "!pip install eegdash"
      ],
      "metadata": {
        "id": "DiMrA-jSk4SK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa355211-7068-4a65-fa27-b3efb543a4f1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting braindecode\n",
            "  Downloading braindecode-1.2.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting mne>=1.10.0 (from braindecode)\n",
            "  Downloading mne-1.10.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting mne_bids>=0.16 (from braindecode)\n",
            "  Downloading mne_bids-0.17.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.16.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from braindecode) (3.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from braindecode) (3.14.0)\n",
            "Collecting skorch>=1.2.0 (from braindecode)\n",
            "  Downloading skorch-1.2.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: torch<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from braindecode) (0.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from braindecode) (1.5.2)\n",
            "Collecting torchinfo~=1.8 (from braindecode)\n",
            "  Downloading torchinfo-1.8.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting wfdb (from braindecode)\n",
            "  Downloading wfdb-4.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting linear_attention_transformer (from braindecode)\n",
            "  Downloading linear_attention_transformer-0.19.1-py3-none-any.whl.metadata (787 bytes)\n",
            "Collecting docstring_inheritance (from braindecode)\n",
            "  Downloading docstring_inheritance-2.2.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (0.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (25.0)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (1.8.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode) (2.9.0.post0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from skorch>=1.2.0->braindecode) (1.6.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.12/dist-packages (from skorch>=1.2.0->braindecode) (0.9.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (3.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode) (3.4.0)\n",
            "Collecting axial-positional-embedding (from linear_attention_transformer->braindecode)\n",
            "  Downloading axial_positional_embedding-0.3.12-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting linformer>=0.1.0 (from linear_attention_transformer->braindecode)\n",
            "  Downloading linformer-0.2.3-py3-none-any.whl.metadata (602 bytes)\n",
            "Collecting local-attention (from linear_attention_transformer->braindecode)\n",
            "  Downloading local_attention-1.11.2-py3-none-any.whl.metadata (929 bytes)\n",
            "Collecting product-key-memory>=0.1.5 (from linear_attention_transformer->braindecode)\n",
            "  Downloading product_key_memory-0.2.11-py3-none-any.whl.metadata (717 bytes)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->braindecode) (2025.2)\n",
            "Requirement already satisfied: aiohttp>=3.10.11 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode) (3.12.15)\n",
            "Collecting pandas (from braindecode)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode) (2.32.4)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode) (0.13.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10.11->wfdb->braindecode) (1.20.1)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne>=1.10.0->braindecode) (4.4.0)\n",
            "Collecting colt5-attention>=0.10.14 (from product-key-memory>=0.1.5->linear_attention_transformer->braindecode)\n",
            "  Downloading CoLT5_attention-0.11.1-py3-none-any.whl.metadata (737 bytes)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->braindecode) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode) (2025.8.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch>=1.2.0->braindecode) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb->braindecode) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.0->braindecode) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne>=1.10.0->braindecode) (3.0.3)\n",
            "Collecting hyper-connections>=0.1.8 (from local-attention->linear_attention_transformer->braindecode)\n",
            "  Downloading hyper_connections-0.2.1-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->braindecode) (2.23)\n",
            "Downloading braindecode-1.2.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.2/305.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mne-1.10.1-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mne_bids-0.17.0-py3-none-any.whl (168 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.9/168.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading skorch-1.2.0-py3-none-any.whl (263 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m263.1/263.1 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\n",
            "Downloading docstring_inheritance-2.2.2-py3-none-any.whl (24 kB)\n",
            "Downloading linear_attention_transformer-0.19.1-py3-none-any.whl (12 kB)\n",
            "Downloading wfdb-4.3.0-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m134.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading linformer-0.2.3-py3-none-any.whl (6.2 kB)\n",
            "Downloading product_key_memory-0.2.11-py3-none-any.whl (6.5 kB)\n",
            "Downloading axial_positional_embedding-0.3.12-py3-none-any.whl (6.7 kB)\n",
            "Downloading local_attention-1.11.2-py3-none-any.whl (9.5 kB)\n",
            "Downloading CoLT5_attention-0.11.1-py3-none-any.whl (18 kB)\n",
            "Downloading hyper_connections-0.2.1-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: torchinfo, docstring_inheritance, pandas, wfdb, skorch, mne, mne_bids, linformer, hyper-connections, axial-positional-embedding, local-attention, colt5-attention, product-key-memory, linear_attention_transformer, braindecode\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.2.2\n",
            "    Uninstalling pandas-2.2.2:\n",
            "      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed axial-positional-embedding-0.3.12 braindecode-1.2.0 colt5-attention-0.11.1 docstring_inheritance-2.2.2 hyper-connections-0.2.1 linear_attention_transformer-0.19.1 linformer-0.2.3 local-attention-1.11.2 mne-1.10.1 mne_bids-0.17.0 pandas-2.3.3 product-key-memory-0.2.11 skorch-1.2.0 torchinfo-1.8.0 wfdb-4.3.0\n",
            "Collecting eegdash\n",
            "  Downloading eegdash-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: braindecode>=1.0 in /usr/local/lib/python3.12/dist-packages (from eegdash) (1.2.0)\n",
            "Requirement already satisfied: mne_bids>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.17.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from eegdash) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from eegdash) (2.3.3)\n",
            "Collecting pybids (from eegdash)\n",
            "  Downloading pybids-0.20.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pymongo (from eegdash)\n",
            "  Downloading pymongo-4.15.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (22 kB)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.12/dist-packages (from eegdash) (1.1.1)\n",
            "Collecting s3fs (from eegdash)\n",
            "  Downloading s3fs-2025.9.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from eegdash) (1.16.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from eegdash) (4.67.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.12/dist-packages (from eegdash) (2025.9.1)\n",
            "Collecting h5io>=0.2.4 (from eegdash)\n",
            "  Downloading h5io-0.2.5-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting pymatreader (from eegdash)\n",
            "  Downloading pymatreader-1.1.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting eeglabio (from eegdash)\n",
            "  Downloading eeglabio-0.1.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (from eegdash) (0.9.0)\n",
            "Requirement already satisfied: docstring_inheritance in /usr/local/lib/python3.12/dist-packages (from eegdash) (2.2.2)\n",
            "Requirement already satisfied: mne>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.10.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (3.10.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (3.14.0)\n",
            "Requirement already satisfied: skorch>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.2.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchaudio<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (2.8.0+cu126)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (0.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.5.2)\n",
            "Requirement already satisfied: torchinfo~=1.8 in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (1.8.0)\n",
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (4.3.0)\n",
            "Requirement already satisfied: linear_attention_transformer in /usr/local/lib/python3.12/dist-packages (from braindecode>=1.0->eegdash) (0.19.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->eegdash) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->eegdash) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->eegdash) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->eegdash) (2025.2)\n",
            "Requirement already satisfied: nibabel>=4.0 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (5.3.2)\n",
            "Collecting formulaic>=0.3 (from pybids->eegdash)\n",
            "  Downloading formulaic-1.2.1-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.31 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (2.0.43)\n",
            "Collecting bids-validator>=1.14.7 (from pybids->eegdash)\n",
            "  Downloading bids_validator-1.14.7.post0-py3-none-any.whl.metadata (39 kB)\n",
            "Collecting num2words>=0.5.10 (from pybids->eegdash)\n",
            "  Downloading num2words-0.5.14-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (8.3.0)\n",
            "Collecting universal_pathlib>=0.2.2 (from pybids->eegdash)\n",
            "  Downloading universal_pathlib-0.3.3-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: frozendict>=2.3 in /usr/local/lib/python3.12/dist-packages (from pybids->eegdash) (2.4.6)\n",
            "Collecting xmltodict (from pymatreader->eegdash)\n",
            "  Downloading xmltodict-1.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting dnspython<3.0.0,>=1.16.0 (from pymongo->eegdash)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting aiobotocore<3.0.0,>=2.5.4 (from s3fs->eegdash)\n",
            "  Downloading aiobotocore-2.24.3-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting fsspec==2025.9.0 (from s3fs->eegdash)\n",
            "  Downloading fsspec-2025.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from s3fs->eegdash) (3.12.15)\n",
            "Requirement already satisfied: packaging>=24.1 in /usr/local/lib/python3.12/dist-packages (from xarray->eegdash) (25.0)\n",
            "Collecting aioitertools<1.0.0,>=0.5.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash)\n",
            "  Downloading aioitertools-0.12.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting botocore<1.40.46,>=1.40.37 (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash)\n",
            "  Downloading botocore-1.40.45-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (6.6.4)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (1.17.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs->eegdash) (1.20.1)\n",
            "Collecting bidsschematools>=0.10 (from bids-validator>=1.14.7->pybids->eegdash)\n",
            "  Downloading bidsschematools-1.1.0-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting interface-meta>=1.2.0 (from formulaic>=0.3->pybids->eegdash)\n",
            "  Downloading interface_meta-1.3.0-py3-none-any.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: narwhals>=1.17 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.3->pybids->eegdash) (2.6.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.12/dist-packages (from formulaic>=0.3->pybids->eegdash) (4.15.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (0.4)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.12/dist-packages (from mne>=1.10.0->braindecode>=1.0->eegdash) (1.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->braindecode>=1.0->eegdash) (3.2.5)\n",
            "Collecting docopt>=0.6.2 (from num2words>=0.5.10->pybids->eegdash)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->eegdash) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from skorch>=1.2.0->braindecode>=1.0->eegdash) (1.6.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.31->pybids->eegdash) (3.2.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.19.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.0->braindecode>=1.0->eegdash) (3.4.0)\n",
            "Collecting pathlib-abc==0.5.1 (from universal_pathlib>=0.2.2->pybids->eegdash)\n",
            "  Downloading pathlib_abc-0.5.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: axial-positional-embedding in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.3.12)\n",
            "Requirement already satisfied: linformer>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.3)\n",
            "Requirement already satisfied: local-attention in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (1.11.2)\n",
            "Requirement already satisfied: product-key-memory>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.11)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode>=1.0->eegdash) (2.32.4)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from wfdb->braindecode>=1.0->eegdash) (0.13.1)\n",
            "Collecting acres (from bidsschematools>=0.10->bids-validator>=1.14.7->pybids->eegdash)\n",
            "  Downloading acres-0.5.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from bidsschematools>=0.10->bids-validator>=1.14.7->pybids->eegdash) (6.0.3)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.40.46,>=1.40.37->aiobotocore<3.0.0,>=2.5.4->s3fs->eegdash) (2.5.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch>=1.5->mne>=1.10.0->braindecode>=1.0->eegdash) (4.4.0)\n",
            "Requirement already satisfied: colt5-attention>=0.10.14 in /usr/local/lib/python3.12/dist-packages (from product-key-memory>=0.1.5->linear_attention_transformer->braindecode>=1.0->eegdash) (0.11.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode>=1.0->eegdash) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode>=1.0->eegdash) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.8.1->wfdb->braindecode>=1.0->eegdash) (2025.8.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.22.0->skorch>=1.2.0->braindecode>=1.0->eegdash) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile>=0.10.0->wfdb->braindecode>=1.0->eegdash) (2.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.0->braindecode>=1.0->eegdash) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->mne>=1.10.0->braindecode>=1.0->eegdash) (3.0.3)\n",
            "Requirement already satisfied: hyper-connections>=0.1.8 in /usr/local/lib/python3.12/dist-packages (from local-attention->linear_attention_transformer->braindecode>=1.0->eegdash) (0.2.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb->braindecode>=1.0->eegdash) (2.23)\n",
            "Downloading eegdash-0.3.8-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/71.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h5io-0.2.5-py3-none-any.whl (17 kB)\n",
            "Downloading eeglabio-0.1.2-py3-none-any.whl (10 kB)\n",
            "Downloading pybids-0.20.0-py3-none-any.whl (215 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m215.7/215.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymatreader-1.1.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading pymongo-4.15.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3fs-2025.9.0-py3-none-any.whl (30 kB)\n",
            "Downloading fsspec-2025.9.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.3/199.3 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiobotocore-2.24.3-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.8/85.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bids_validator-1.14.7.post0-py3-none-any.whl (23 kB)\n",
            "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m331.1/331.1 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading formulaic-1.2.1-py3-none-any.whl (117 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.3/117.3 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading num2words-0.5.14-py3-none-any.whl (163 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading universal_pathlib-0.3.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathlib_abc-0.5.1-py3-none-any.whl (20 kB)\n",
            "Downloading xmltodict-1.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading aioitertools-0.12.0-py3-none-any.whl (24 kB)\n",
            "Downloading bidsschematools-1.1.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.2/179.2 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.40.45-py3-none-any.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading interface_meta-1.3.0-py3-none-any.whl (14 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading acres-0.5.0-py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=7afc86dc67f6245d4dba4bbb8735b269d2712faa37bd68385a86088d8009cfdf\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "Successfully built docopt\n",
            "Installing collected packages: docopt, xmltodict, pathlib-abc, num2words, jmespath, interface-meta, fsspec, dnspython, aioitertools, acres, universal_pathlib, pymongo, pymatreader, h5io, eeglabio, botocore, bidsschematools, formulaic, bids-validator, aiobotocore, s3fs, pybids, eegdash\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.0\n",
            "    Uninstalling fsspec-2025.3.0:\n",
            "      Successfully uninstalled fsspec-2025.3.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 4.0.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.9.0 which is incompatible.\n",
            "dask-cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "cudf-cu12 25.6.0 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.3 which is incompatible.\n",
            "gcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed acres-0.5.0 aiobotocore-2.24.3 aioitertools-0.12.0 bids-validator-1.14.7.post0 bidsschematools-1.1.0 botocore-1.40.45 dnspython-2.8.0 docopt-0.6.2 eegdash-0.3.8 eeglabio-0.1.2 formulaic-1.2.1 fsspec-2025.9.0 h5io-0.2.5 interface-meta-1.3.0 jmespath-1.0.1 num2words-0.5.14 pathlib-abc-0.5.1 pybids-0.20.0 pymatreader-1.1.0 pymongo-4.15.3 s3fs-2025.9.0 universal_pathlib-0.3.3 xmltodict-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import optim\n",
        "from torch.nn.functional import l1_loss\n",
        "from braindecode.preprocessing import preprocess, Preprocessor, create_fixed_length_windows, create_windows_from_events\n",
        "from braindecode.datasets.base import EEGWindowsDataset, BaseConcatDataset, BaseDataset\n",
        "from braindecode.models import EEGNeX\n",
        "from eegdash import EEGChallengeDataset\n",
        "from typing import Optional\n",
        "from torch.nn import Module\n",
        "from torch.optim.lr_scheduler import LRScheduler\n",
        "from tqdm import tqdm\n",
        "from eegdash.dataset import EEGChallengeDataset\n",
        "from eegdash.hbn.windows import annotate_trials_with_target, add_aux_anchors, add_extras_columns, keep_only_recordings_with"
      ],
      "metadata": {
        "id": "P2fFM-Dlk2VZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZa1UpBSj_Pq",
        "outputId": "b0704297-13aa-4587-8932-54483c14b196"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA-enabled GPU found. Training should be faster.\n"
          ]
        }
      ],
      "source": [
        "# Identify whether a CUDA-enabled GPU is available\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if device == \"cuda\":\n",
        "    msg ='CUDA-enabled GPU found. Training should be faster.'\n",
        "else:\n",
        "    msg = (\n",
        "        \"No GPU found. Training will be carried out on CPU, which might be \"\n",
        "        \"slower.\\n\\nIf running on Google Colab, you can request a GPU runtime by\"\n",
        "        \" clicking\\n`Runtime/Change runtime type` in the top bar menu, then \"\n",
        "        \"selecting \\'T4 GPU\\'\\nunder \\'Hardware accelerator\\'.\"\n",
        "    )\n",
        "print(msg)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "release_list = ['R1', 'R2', 'R3', 'R4', 'R5', 'R6',\n",
        "                'R7', 'R8', 'R9', 'R10', 'R11']\n",
        "train_set = []\n",
        "valid_set = []\n",
        "test_set = []\n",
        "\n",
        "DATA_DIR = Path('data')\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "_s57mK56kkDd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for release in tqdm(release_list):\n",
        "  dataset_ccd = EEGChallengeDataset(\n",
        "      task = 'contrastChangeDetection',\n",
        "      release = release,\n",
        "      cache_dir = DATA_DIR,\n",
        "      mini = True\n",
        "  )\n",
        "  raws = Parallel(n_jobs=-1)(\n",
        "      delayed(lambda d: d.raw)(d) for d in dataset_ccd.datasets\n",
        "  )\n",
        "\n",
        "  EPOCH_LEN_S = 2.0\n",
        "  SFREQ = 100\n",
        "\n",
        "  transformation_offline = [\n",
        "      Preprocessor(\n",
        "          annotate_trials_with_target,\n",
        "          target_field = 'rt_from_stimulus', epoch_length =  EPOCH_LEN_S,\n",
        "          require_stimulus = True,\n",
        "          require_response = True,\n",
        "          apply_on_array = False,\n",
        "      ),\n",
        "      Preprocessor(add_aux_anchors, apply_on_array=False),\n",
        "  ]\n",
        "  preprocess (dataset_ccd, transformation_offline, n_jobs=1)\n",
        "\n",
        "  ANCHOR = 'stimulus_anchor'\n",
        "  SHIFT_AFTER_STIM = 0.5\n",
        "  WINDOW_LEN = 2.0\n",
        "\n",
        "  dataset = keep_only_recordings_with(ANCHOR, dataset_ccd)\n",
        "\n",
        "  single_windows = create_windows_from_events(\n",
        "      dataset,\n",
        "      mapping={ANCHOR: 0},\n",
        "      trial_start_offset_samples = int(SHIFT_AFTER_STIM * SFREQ),\n",
        "      trial_stop_offset_samples = int((SHIFT_AFTER_STIM + WINDOW_LEN) * SFREQ),\n",
        "      window_size_samples = int(EPOCH_LEN_S * SFREQ),\n",
        "      window_stride_samples = SFREQ,\n",
        "      preload = True,\n",
        "  )\n",
        "\n",
        "  single_windows = add_extras_columns(\n",
        "      single_windows,\n",
        "      dataset,\n",
        "      desc = ANCHOR, keys = (\"target\", \"rt_from_stimulus\", \"rt_from_trialstart\",\n",
        "            \"stimulus_onset\", \"response_onset\", \"correct\", \"response_type\"))\n",
        "\n",
        "  meta_information = single_windows.get_metadata()\n",
        "\n",
        "  subjects = meta_information['subject'].unique()\n",
        "  sub_rm = [\"NDARWV769JM7\", \"NDARME789TD2\", \"NDARUA442ZVF\", \"NDARJP304NK1\",\n",
        "            \"NDARTY128YLU\", \"NDARDW550GU6\", \"NDARLD243KRE\", \"NDARUJ292JXV\", \"NDARBA381JGH\"]\n",
        "  subjects = [s for s in subjects if s not in sub_rm]\n",
        "\n",
        "  subject_split = single_windows.split('subject')\n",
        "\n",
        "  if release != 'R5':\n",
        "    for s in subject_split:\n",
        "      train_set.append(subject_split[s])\n",
        "  else:\n",
        "    for s in subject_split:\n",
        "      valid_set.append(subject_split[s])"
      ],
      "metadata": {
        "id": "qPLErRCBkoiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30628bd6-5cfe-4434-ffa2-bff66cf102c1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/11 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  9%|▉         | 1/11 [03:56<39:20, 236.03s/it]/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 18%|█▊        | 2/11 [07:35<33:54, 226.08s/it]/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 3/11 [11:10<29:30, 221.26s/it]/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 36%|███▋      | 4/11 [14:34<25:00, 214.41s/it]/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 45%|████▌     | 5/11 [17:53<20:52, 208.73s/it]/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 55%|█████▍    | 6/11 [21:10<17:03, 204.71s/it]/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 64%|██████▎   | 7/11 [24:36<13:41, 205.25s/it]/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 73%|███████▎  | 8/11 [28:04<10:18, 206.05s/it]/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 82%|████████▏ | 9/11 [31:30<06:52, 206.07s/it]/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n",
            "WARNING:root:Recording /content/data/ds005515-bdf-mini/sub-NDARKM061NHZ/eeg/sub-NDARKM061NHZ_task-contrastChangeDetection_run-2_eeg.bdf does not contain event 'stimulus_anchor'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 91%|█████████ | 10/11 [34:45<03:22, 202.73s/it]/usr/local/lib/python3.12/dist-packages/eegdash/dataset/dataset.py:126: UserWarning: \n",
            "\n",
            "[EEGChallengeDataset] EEG 2025 Competition Data Notice:\n",
            "-------------------------------------------------------\n",
            "This object loads the HBN dataset that has been preprocessed for the EEG Challenge:\n",
            "  - Downsampled from 500Hz to 100Hz\n",
            "  - Bandpass filtered (0.5–50 Hz)\n",
            "\n",
            "For full preprocessing details, see:\n",
            "  https://github.com/eeg2025/downsample-datasets\n",
            "\n",
            "IMPORTANT: The data accessed via `EEGChallengeDataset` is NOT identical to what you get from `EEGDashDataset` directly.\n",
            "If you are participating in the competition, always use `EEGChallengeDataset` to ensure consistency with the challenge data.\n",
            "\n",
            "\n",
            "  warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n",
            "Used Annotations descriptions: [np.str_('stimulus_anchor')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11/11 [37:56<00:00, 206.98s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = BaseConcatDataset(train_set)\n",
        "valid_set = BaseConcatDataset(valid_set)\n",
        "\n",
        "print(f'Train set: {len(train_set)}')\n",
        "print(f'Valid set: {len(valid_set)}')"
      ],
      "metadata": {
        "id": "1LLxt7B5wlHL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff2b9bf0-4c80-40ae-8fdb-cf131d321a2b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train set: 12805\n",
            "Valid set: 1214\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Build model"
      ],
      "metadata": {
        "id": "YR2u7CQ_W8Yk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "num_workers = 2\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "metadata": {
        "id": "Dx8xegvoW7wI"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from braindecode.models.util import models_dict\n",
        "\n",
        "names = sorted(models_dict)\n",
        "w = max(len(n) for n in names)\n",
        "\n",
        "for i in range(0, len(names), 3):\n",
        "    row = names[i:i+3]\n",
        "    print(\"  \".join(f\"{n:<{w}}\" for n in row))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-0PtzZzXPLg",
        "outputId": "5702b3df-5116-417e-c65b-ca6fecf7f84b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATCNet                  AttentionBaseNet        AttnSleep             \n",
            "BDTCN                   BIOT                    CTNet                 \n",
            "ContraWR                Deep4Net                DeepSleepNet          \n",
            "EEGConformer            EEGITNet                EEGInceptionERP       \n",
            "EEGInceptionMI          EEGMiner                EEGNeX                \n",
            "EEGNet                  EEGSimpleConv           EEGTCNet              \n",
            "FBCNet                  FBLightConvNet          FBMSNet               \n",
            "IFNet                   Labram                  MSVTNet               \n",
            "SCCNet                  SPARCNet                ShallowFBCSPNet       \n",
            "SignalJEPA              SignalJEPA_Contextual   SignalJEPA_PostLocal  \n",
            "SignalJEPA_PreLocal     SincShallowNet          SleepStagerBlanco2020 \n",
            "SleepStagerChambon2018  SyncNet                 TIDNet                \n",
            "TSception               USleep                \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = EEGNeX(n_chans=129,\n",
        "               n_outputs=1,\n",
        "               n_times=200,\n",
        "               sfreq=100)"
      ],
      "metadata": {
        "id": "9DrSK37pXffi"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model training"
      ],
      "metadata": {
        "id": "hVv6f86CTwCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-3\n",
        "weight_decay = 1e-5\n",
        "n_epochs = 60\n",
        "early_stopping_patience = 50"
      ],
      "metadata": {
        "id": "X0LWF6rSTzx1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(\n",
        "    dataloader: DataLoader,\n",
        "    model: Module,\n",
        "    loss_fn,\n",
        "    optimizer,\n",
        "    scheduler: Optional[LRScheduler],\n",
        "    epoch: int,\n",
        "    device,\n",
        "    print_batch_stats: bool = True,):\n",
        "  model.train()\n",
        "\n",
        "  total_loss = 0.0\n",
        "  sum_sq_err = 0.0\n",
        "  n_samples = 0\n",
        "\n",
        "  progress_bar = tqdm(\n",
        "      enumerate(dataloader),\n",
        "      total=len(dataloader),\n",
        "      disable=not print_batch_stats)\n",
        "  for batch_idx, batch in progress_bar:\n",
        "    X, y = batch[0], batch[1]\n",
        "    X, y = X.to(device).float(), y.to(device).float()\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    preds = model(X)\n",
        "    loss = loss_fn(preds, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    preds_flat = preds.detach().view(-1)\n",
        "    y_flat = y.detach().view(-1)\n",
        "    sum_sq_err += torch.sum((preds_flat - y_flat)**2).item()\n",
        "    n_samples += y_flat.numel()\n",
        "\n",
        "    if print_batch_stats:\n",
        "      running_rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n",
        "      progress_bar.set_description(\n",
        "          f\"Epoch {epoch}, Batch {batch_idx + 1}/{len(dataloader)},\"\n",
        "          f\"Loss: {loss.item():.6f}, RMSE: {running_rmse:.6f}\"\n",
        "      )\n",
        "  if scheduler is not None:\n",
        "    scheduler.step()\n",
        "\n",
        "  avg_loss = total_loss/len(dataloader)\n",
        "  rmse = (sum_sq_err/max(n_samples, 1))** 0.5\n",
        "  return avg_loss, rmse"
      ],
      "metadata": {
        "id": "_gg9UwRKUNfH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  def valid_model(\n",
        "      dataloader: DataLoader,\n",
        "      model: Module,\n",
        "      loss_fn,\n",
        "      device,\n",
        "      print_batch_stats: bool = True):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    sum_sq_err = 0.0\n",
        "    n_batches = len(dataloader)\n",
        "    n_samples = 0.0\n",
        "\n",
        "    iterator = tqdm(\n",
        "        enumerate(dataloader),\n",
        "        total=len(dataloader),\n",
        "        disable=not print_batch_stats)\n",
        "\n",
        "    for batch_idx, batch in iterator:\n",
        "      X, y = batch[0], batch[1]\n",
        "      X, y = X.to(device).float(), y.to(device).float()\n",
        "\n",
        "      preds = model(X)\n",
        "      batch_loss = loss_fn(preds, y).item()\n",
        "      total_loss += batch_loss\n",
        "\n",
        "      preds_flat = preds.detach().view(-1)\n",
        "      y_flat = y.detach().view(-1)\n",
        "      sum_sq_err += torch.sum((preds_flat - y_flat)**2).item()\n",
        "      n_samples += y_flat.numel()\n",
        "\n",
        "      if print_batch_stats:\n",
        "        running_rmse = (sum_sq_err/max(n_samples,1)) ** 0.5\n",
        "        iterator.set_description(\n",
        "            f\"Val Batch {batch_idx + 1}/{len(dataloader)},\"\n",
        "            f\"Loss: {batch_loss:.6f}, RMSE: {running_rmse:.6f}\"\n",
        "        )\n",
        "\n",
        "    avg_loss = total_loss / n_batches if n_batches else float(\"nan\")\n",
        "    rmse = (sum_sq_err / max(n_samples, 1)) ** 0.5\n",
        "\n",
        "    print(f\"Val RMSE: {rmse:.6f}, Val Loss: {avg_loss:.6f}\")\n",
        "    return avg_loss, rmse"
      ],
      "metadata": {
        "id": "xuaOp1XF2R4g"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max = n_epochs -1)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "patience = 5\n",
        "min_delta = 1e-4\n",
        "best_rmse = float('inf')\n",
        "epochs_no_improve = 0\n",
        "best_state, best_epoch = None, None\n",
        "\n",
        "model.to(device)\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  print(f'Epoch {epoch}/{n_epochs}: ', end='')\n",
        "\n",
        "  train_loss, train_rmse = train_one_epoch(\n",
        "      train_loader, model, loss_fn, optimizer, scheduler, epoch, device\n",
        "  )\n",
        "  valid_loss, valid_rmse = valid_model(\n",
        "      valid_loader, model, loss_fn, device\n",
        "  )\n",
        "  print(\n",
        "      f'Train RMSE: {train_rmse:.6f}, '\n",
        "      f'Average Train Loss: {train_loss:.6f}, '\n",
        "      f'Val RMSE: {valid_rmse:.6f}, '\n",
        "      f'Average Valid Loss: {valid_loss:.6f}\\n'\n",
        "  )\n",
        "if best_state is not None:\n",
        "  model.load_state_dict(best_state)"
      ],
      "metadata": {
        "id": "C61B7y3g5YlP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fe92613-050c-4610-bf68-3c9be10cb4c1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/101 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py:543: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /pytorch/aten/src/ATen/native/Convolution.cpp:1027.)\n",
            "  return F.conv2d(\n",
            "Epoch 1, Batch 101/101,Loss: 0.100190, RMSE: 0.835295: 100%|██████████| 101/101 [01:01<00:00,  1.65it/s]\n",
            "Val Batch 10/10,Loss: 0.386997, RMSE: 0.472473: 100%|██████████| 10/10 [00:04<00:00,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.472473, Val Loss: 0.231675\n",
            "Train RMSE: 0.835295, Average Train Loss: 0.692033, Val RMSE: 0.472473, Average Valid Loss: 0.231675\n",
            "\n",
            "Epoch 2/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 2, Batch 101/101,Loss: 0.393672, RMSE: 0.481925: 100%|██████████| 101/101 [00:57<00:00,  1.75it/s]\n",
            "Val Batch 10/10,Loss: 0.406114, RMSE: 0.565627: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.565627, Val Loss: 0.324377\n",
            "Train RMSE: 0.481925, Average Train Loss: 0.233787, Val RMSE: 0.565627, Average Valid Loss: 0.324377\n",
            "\n",
            "Epoch 3/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 3, Batch 101/101,Loss: 0.261439, RMSE: 0.466932: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.342528, RMSE: 0.450396: 100%|██████████| 10/10 [00:04<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.450396, Val Loss: 0.210058\n",
            "Train RMSE: 0.466932, Average Train Loss: 0.218438, Val RMSE: 0.450396, Average Valid Loss: 0.210058\n",
            "\n",
            "Epoch 4/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 4, Batch 101/101,Loss: 0.342430, RMSE: 0.457216: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.288806, RMSE: 0.421368: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.421368, Val Loss: 0.183288\n",
            "Train RMSE: 0.457216, Average Train Loss: 0.210316, Val RMSE: 0.421368, Average Valid Loss: 0.183288\n",
            "\n",
            "Epoch 5/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 5, Batch 101/101,Loss: 0.084066, RMSE: 0.453886: 100%|██████████| 101/101 [00:58<00:00,  1.74it/s]\n",
            "Val Batch 10/10,Loss: 0.288473, RMSE: 0.421260: 100%|██████████| 10/10 [00:04<00:00,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.421260, Val Loss: 0.183184\n",
            "Train RMSE: 0.453886, Average Train Loss: 0.204853, Val RMSE: 0.421260, Average Valid Loss: 0.183184\n",
            "\n",
            "Epoch 6/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 6, Batch 101/101,Loss: 0.070230, RMSE: 0.456445: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.260011, RMSE: 0.402845: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.402845, Val Loss: 0.167323\n",
            "Train RMSE: 0.456445, Average Train Loss: 0.207028, Val RMSE: 0.402845, Average Valid Loss: 0.167323\n",
            "\n",
            "Epoch 7/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 7, Batch 101/101,Loss: 0.382325, RMSE: 0.450983: 100%|██████████| 101/101 [00:56<00:00,  1.78it/s]\n",
            "Val Batch 10/10,Loss: 0.269894, RMSE: 0.410211: 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.410211, Val Loss: 0.173513\n",
            "Train RMSE: 0.450983, Average Train Loss: 0.205088, Val RMSE: 0.410211, Average Valid Loss: 0.173513\n",
            "\n",
            "Epoch 8/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 8, Batch 101/101,Loss: 0.314686, RMSE: 0.448365: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.277724, RMSE: 0.416971: 100%|██████████| 10/10 [00:04<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.416971, Val Loss: 0.179220\n",
            "Train RMSE: 0.448365, Average Train Loss: 0.202113, Val RMSE: 0.416971, Average Valid Loss: 0.179220\n",
            "\n",
            "Epoch 9/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 9, Batch 101/101,Loss: 0.121968, RMSE: 0.449241: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.286668, RMSE: 0.423491: 100%|██████████| 10/10 [00:04<00:00,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.423491, Val Loss: 0.184879\n",
            "Train RMSE: 0.449241, Average Train Loss: 0.201058, Val RMSE: 0.423491, Average Valid Loss: 0.184879\n",
            "\n",
            "Epoch 10/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 10, Batch 101/101,Loss: 0.099054, RMSE: 0.448572: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.212647, RMSE: 0.385667: 100%|██████████| 10/10 [00:03<00:00,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.385667, Val Loss: 0.152035\n",
            "Train RMSE: 0.448572, Average Train Loss: 0.200245, Val RMSE: 0.385667, Average Valid Loss: 0.152035\n",
            "\n",
            "Epoch 11/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 11, Batch 101/101,Loss: 0.101418, RMSE: 0.444538: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.229645, RMSE: 0.393662: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.393662, Val Loss: 0.158820\n",
            "Train RMSE: 0.444538, Average Train Loss: 0.196698, Val RMSE: 0.393662, Average Valid Loss: 0.158820\n",
            "\n",
            "Epoch 12/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 12, Batch 101/101,Loss: 0.148758, RMSE: 0.444823: 100%|██████████| 101/101 [00:55<00:00,  1.83it/s]\n",
            "Val Batch 10/10,Loss: 0.347513, RMSE: 0.456223: 100%|██████████| 10/10 [00:04<00:00,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.456223, Val Loss: 0.215325\n",
            "Train RMSE: 0.444823, Average Train Loss: 0.197400, Val RMSE: 0.456223, Average Valid Loss: 0.215325\n",
            "\n",
            "Epoch 13/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 13, Batch 101/101,Loss: 0.109162, RMSE: 0.441627: 100%|██████████| 101/101 [00:56<00:00,  1.77it/s]\n",
            "Val Batch 10/10,Loss: 0.245746, RMSE: 0.403887: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.403887, Val Loss: 0.167385\n",
            "Train RMSE: 0.441627, Average Train Loss: 0.194218, Val RMSE: 0.403887, Average Valid Loss: 0.167385\n",
            "\n",
            "Epoch 14/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 14, Batch 101/101,Loss: 0.125944, RMSE: 0.443607: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.364153, RMSE: 0.463417: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.463417, Val Loss: 0.222459\n",
            "Train RMSE: 0.443607, Average Train Loss: 0.196113, Val RMSE: 0.463417, Average Valid Loss: 0.222459\n",
            "\n",
            "Epoch 15/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15, Batch 101/101,Loss: 0.423374, RMSE: 0.440861: 100%|██████████| 101/101 [01:01<00:00,  1.65it/s]\n",
            "Val Batch 10/10,Loss: 0.291044, RMSE: 0.433685: 100%|██████████| 10/10 [00:04<00:00,  2.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.433685, Val Loss: 0.193392\n",
            "Train RMSE: 0.440861, Average Train Loss: 0.196537, Val RMSE: 0.433685, Average Valid Loss: 0.193392\n",
            "\n",
            "Epoch 16/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 16, Batch 101/101,Loss: 0.182535, RMSE: 0.441643: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.353683, RMSE: 0.452055: 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.452055, Val Loss: 0.212053\n",
            "Train RMSE: 0.441643, Average Train Loss: 0.194930, Val RMSE: 0.452055, Average Valid Loss: 0.212053\n",
            "\n",
            "Epoch 17/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 17, Batch 101/101,Loss: 0.273051, RMSE: 0.438752: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.260981, RMSE: 0.409749: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.409749, Val Loss: 0.172694\n",
            "Train RMSE: 0.438752, Average Train Loss: 0.193270, Val RMSE: 0.409749, Average Valid Loss: 0.172694\n",
            "\n",
            "Epoch 18/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 18, Batch 101/101,Loss: 0.324881, RMSE: 0.438041: 100%|██████████| 101/101 [00:57<00:00,  1.74it/s]\n",
            "Val Batch 10/10,Loss: 0.256564, RMSE: 0.412481: 100%|██████████| 10/10 [00:04<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.412481, Val Loss: 0.174597\n",
            "Train RMSE: 0.438041, Average Train Loss: 0.193145, Val RMSE: 0.412481, Average Valid Loss: 0.174597\n",
            "\n",
            "Epoch 19/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 19, Batch 101/101,Loss: 0.157246, RMSE: 0.437105: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.268211, RMSE: 0.412867: 100%|██████████| 10/10 [00:04<00:00,  2.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.412867, Val Loss: 0.175499\n",
            "Train RMSE: 0.437105, Average Train Loss: 0.190739, Val RMSE: 0.412867, Average Valid Loss: 0.175499\n",
            "\n",
            "Epoch 20/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 20, Batch 101/101,Loss: 0.276435, RMSE: 0.434603: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.440093, RMSE: 0.509441: 100%|██████████| 10/10 [00:04<00:00,  2.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.509441, Val Loss: 0.268840\n",
            "Train RMSE: 0.434603, Average Train Loss: 0.189713, Val RMSE: 0.509441, Average Valid Loss: 0.268840\n",
            "\n",
            "Epoch 21/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 21, Batch 101/101,Loss: 0.111145, RMSE: 0.433625: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.371257, RMSE: 0.469409: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.469409, Val Loss: 0.228126\n",
            "Train RMSE: 0.433625, Average Train Loss: 0.187299, Val RMSE: 0.469409, Average Valid Loss: 0.228126\n",
            "\n",
            "Epoch 22/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22, Batch 101/101,Loss: 0.163692, RMSE: 0.430763: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.223247, RMSE: 0.390566: 100%|██████████| 10/10 [00:04<00:00,  2.29it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.390566, Val Loss: 0.156187\n",
            "Train RMSE: 0.430763, Average Train Loss: 0.185349, Val RMSE: 0.390566, Average Valid Loss: 0.156187\n",
            "\n",
            "Epoch 23/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 23, Batch 101/101,Loss: 0.215593, RMSE: 0.432701: 100%|██████████| 101/101 [00:57<00:00,  1.76it/s]\n",
            "Val Batch 10/10,Loss: 0.393787, RMSE: 0.481112: 100%|██████████| 10/10 [00:04<00:00,  2.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.481112, Val Loss: 0.239838\n",
            "Train RMSE: 0.432701, Average Train Loss: 0.187500, Val RMSE: 0.481112, Average Valid Loss: 0.239838\n",
            "\n",
            "Epoch 24/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 24, Batch 101/101,Loss: 0.116605, RMSE: 0.430252: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.268863, RMSE: 0.412591: 100%|██████████| 10/10 [00:03<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.412591, Val Loss: 0.175317\n",
            "Train RMSE: 0.430252, Average Train Loss: 0.184465, Val RMSE: 0.412591, Average Valid Loss: 0.175317\n",
            "\n",
            "Epoch 25/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 25, Batch 101/101,Loss: 0.563872, RMSE: 0.426966: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.350069, RMSE: 0.458831: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.458831, Val Loss: 0.217721\n",
            "Train RMSE: 0.426966, Average Train Loss: 0.185930, Val RMSE: 0.458831, Average Valid Loss: 0.217721\n",
            "\n",
            "Epoch 26/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 26, Batch 101/101,Loss: 0.458809, RMSE: 0.427916: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.352659, RMSE: 0.455296: 100%|██████████| 10/10 [00:04<00:00,  2.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.455296, Val Loss: 0.214790\n",
            "Train RMSE: 0.427916, Average Train Loss: 0.185736, Val RMSE: 0.455296, Average Valid Loss: 0.214790\n",
            "\n",
            "Epoch 27/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27, Batch 101/101,Loss: 0.546035, RMSE: 0.427827: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.310504, RMSE: 0.437184: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.437184, Val Loss: 0.197285\n",
            "Train RMSE: 0.427827, Average Train Loss: 0.186490, Val RMSE: 0.437184, Average Valid Loss: 0.197285\n",
            "\n",
            "Epoch 28/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 28, Batch 101/101,Loss: 0.179219, RMSE: 0.423687: 100%|██████████| 101/101 [00:55<00:00,  1.83it/s]\n",
            "Val Batch 10/10,Loss: 0.379700, RMSE: 0.468707: 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.468707, Val Loss: 0.227937\n",
            "Train RMSE: 0.423687, Average Train Loss: 0.179508, Val RMSE: 0.468707, Average Valid Loss: 0.227937\n",
            "\n",
            "Epoch 29/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 29, Batch 101/101,Loss: 0.210751, RMSE: 0.421954: 100%|██████████| 101/101 [00:57<00:00,  1.77it/s]\n",
            "Val Batch 10/10,Loss: 0.189174, RMSE: 0.382269: 100%|██████████| 10/10 [00:04<00:00,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.382269, Val Loss: 0.148349\n",
            "Train RMSE: 0.421954, Average Train Loss: 0.178357, Val RMSE: 0.382269, Average Valid Loss: 0.148349\n",
            "\n",
            "Epoch 30/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 30, Batch 101/101,Loss: 0.068024, RMSE: 0.423878: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.228555, RMSE: 0.389884: 100%|██████████| 10/10 [00:04<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.389884, Val Loss: 0.155956\n",
            "Train RMSE: 0.423878, Average Train Loss: 0.178611, Val RMSE: 0.389884, Average Valid Loss: 0.155956\n",
            "\n",
            "Epoch 31/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 31, Batch 101/101,Loss: 0.128527, RMSE: 0.419782: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.447855, RMSE: 0.515496: 100%|██████████| 10/10 [00:03<00:00,  2.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.515496, Val Loss: 0.275126\n",
            "Train RMSE: 0.419782, Average Train Loss: 0.175763, Val RMSE: 0.515496, Average Valid Loss: 0.275126\n",
            "\n",
            "Epoch 32/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 32, Batch 101/101,Loss: 0.085398, RMSE: 0.420635: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.435587, RMSE: 0.504735: 100%|██████████| 10/10 [00:04<00:00,  2.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.504735, Val Loss: 0.264081\n",
            "Train RMSE: 0.420635, Average Train Loss: 0.176063, Val RMSE: 0.504735, Average Valid Loss: 0.264081\n",
            "\n",
            "Epoch 33/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 33, Batch 101/101,Loss: 0.622974, RMSE: 0.416674: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.295548, RMSE: 0.431739: 100%|██████████| 10/10 [00:04<00:00,  2.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.431739, Val Loss: 0.192027\n",
            "Train RMSE: 0.416674, Average Train Loss: 0.177892, Val RMSE: 0.431739, Average Valid Loss: 0.192027\n",
            "\n",
            "Epoch 34/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 34, Batch 101/101,Loss: 0.117349, RMSE: 0.417024: 100%|██████████| 101/101 [00:58<00:00,  1.74it/s]\n",
            "Val Batch 10/10,Loss: 0.313773, RMSE: 0.439183: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.439183, Val Loss: 0.199115\n",
            "Train RMSE: 0.417024, Average Train Loss: 0.173371, Val RMSE: 0.439183, Average Valid Loss: 0.199115\n",
            "\n",
            "Epoch 35/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 35, Batch 101/101,Loss: 0.055493, RMSE: 0.415480: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.180787, RMSE: 0.388449: 100%|██████████| 10/10 [00:04<00:00,  2.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.388449, Val Loss: 0.152434\n",
            "Train RMSE: 0.415480, Average Train Loss: 0.171510, Val RMSE: 0.388449, Average Valid Loss: 0.152434\n",
            "\n",
            "Epoch 36/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 36, Batch 101/101,Loss: 0.033707, RMSE: 0.413262: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.195160, RMSE: 0.386086: 100%|██████████| 10/10 [00:04<00:00,  2.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.386086, Val Loss: 0.151439\n",
            "Train RMSE: 0.413262, Average Train Loss: 0.169481, Val RMSE: 0.386086, Average Valid Loss: 0.151439\n",
            "\n",
            "Epoch 37/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 37, Batch 101/101,Loss: 0.041410, RMSE: 0.413638: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.436170, RMSE: 0.508610: 100%|██████████| 10/10 [00:04<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.508610, Val Loss: 0.267836\n",
            "Train RMSE: 0.413638, Average Train Loss: 0.169862, Val RMSE: 0.508610, Average Valid Loss: 0.267836\n",
            "\n",
            "Epoch 38/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38, Batch 101/101,Loss: 0.161555, RMSE: 0.415231: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.240601, RMSE: 0.400645: 100%|██████████| 10/10 [00:04<00:00,  2.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.400645, Val Loss: 0.164646\n",
            "Train RMSE: 0.415231, Average Train Loss: 0.172314, Val RMSE: 0.400645, Average Valid Loss: 0.164646\n",
            "\n",
            "Epoch 39/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 39, Batch 101/101,Loss: 0.348121, RMSE: 0.411009: 100%|██████████| 101/101 [00:57<00:00,  1.76it/s]\n",
            "Val Batch 10/10,Loss: 0.207497, RMSE: 0.385085: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.385085, Val Loss: 0.151343\n",
            "Train RMSE: 0.411009, Average Train Loss: 0.170633, Val RMSE: 0.385085, Average Valid Loss: 0.151343\n",
            "\n",
            "Epoch 40/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 40, Batch 101/101,Loss: 0.093213, RMSE: 0.414814: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.258955, RMSE: 0.406485: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.406485, Val Loss: 0.170063\n",
            "Train RMSE: 0.414814, Average Train Loss: 0.171320, Val RMSE: 0.406485, Average Valid Loss: 0.170063\n",
            "\n",
            "Epoch 41/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 41, Batch 101/101,Loss: 0.386820, RMSE: 0.412514: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.328005, RMSE: 0.450351: 100%|██████████| 10/10 [00:04<00:00,  2.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.450351, Val Loss: 0.209271\n",
            "Train RMSE: 0.412514, Average Train Loss: 0.172229, Val RMSE: 0.450351, Average Valid Loss: 0.209271\n",
            "\n",
            "Epoch 42/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 42, Batch 101/101,Loss: 0.085966, RMSE: 0.408837: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.198991, RMSE: 0.395818: 100%|██████████| 10/10 [00:04<00:00,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.395818, Val Loss: 0.158854\n",
            "Train RMSE: 0.408837, Average Train Loss: 0.166376, Val RMSE: 0.395818, Average Valid Loss: 0.158854\n",
            "\n",
            "Epoch 43/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 43, Batch 101/101,Loss: 0.351644, RMSE: 0.409057: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.283186, RMSE: 0.422844: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.422844, Val Loss: 0.184180\n",
            "Train RMSE: 0.409057, Average Train Loss: 0.169081, Val RMSE: 0.422844, Average Valid Loss: 0.184180\n",
            "\n",
            "Epoch 44/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 44, Batch 101/101,Loss: 0.015487, RMSE: 0.407295: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.279677, RMSE: 0.428880: 100%|██████████| 10/10 [00:05<00:00,  1.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.428880, Val Loss: 0.188875\n",
            "Train RMSE: 0.407295, Average Train Loss: 0.164459, Val RMSE: 0.428880, Average Valid Loss: 0.188875\n",
            "\n",
            "Epoch 45/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 45, Batch 101/101,Loss: 0.355826, RMSE: 0.406647: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.359943, RMSE: 0.472012: 100%|██████████| 10/10 [00:04<00:00,  2.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.472012, Val Loss: 0.229867\n",
            "Train RMSE: 0.406647, Average Train Loss: 0.167174, Val RMSE: 0.472012, Average Valid Loss: 0.229867\n",
            "\n",
            "Epoch 46/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46, Batch 101/101,Loss: 0.358052, RMSE: 0.405777: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.204168, RMSE: 0.395825: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.395825, Val Loss: 0.159126\n",
            "Train RMSE: 0.405777, Average Train Loss: 0.166495, Val RMSE: 0.395825, Average Valid Loss: 0.159126\n",
            "\n",
            "Epoch 47/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 47, Batch 101/101,Loss: 0.102576, RMSE: 0.404621: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.252656, RMSE: 0.413466: 100%|██████████| 10/10 [00:04<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.413466, Val Loss: 0.175167\n",
            "Train RMSE: 0.404621, Average Train Loss: 0.163137, Val RMSE: 0.413466, Average Valid Loss: 0.175167\n",
            "\n",
            "Epoch 48/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48, Batch 101/101,Loss: 0.050206, RMSE: 0.403430: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.262419, RMSE: 0.415678: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.415678, Val Loss: 0.177410\n",
            "Train RMSE: 0.403430, Average Train Loss: 0.161685, Val RMSE: 0.415678, Average Valid Loss: 0.177410\n",
            "\n",
            "Epoch 49/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 49, Batch 101/101,Loss: 0.027055, RMSE: 0.401609: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.182109, RMSE: 0.393175: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.393175, Val Loss: 0.156006\n",
            "Train RMSE: 0.401609, Average Train Loss: 0.160013, Val RMSE: 0.393175, Average Valid Loss: 0.156006\n",
            "\n",
            "Epoch 50/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 50, Batch 101/101,Loss: 0.446765, RMSE: 0.402685: 100%|██████████| 101/101 [00:57<00:00,  1.74it/s]\n",
            "Val Batch 10/10,Loss: 0.277647, RMSE: 0.425970: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.425970, Val Loss: 0.186411\n",
            "Train RMSE: 0.402685, Average Train Loss: 0.164863, Val RMSE: 0.425970, Average Valid Loss: 0.186411\n",
            "\n",
            "Epoch 51/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 51, Batch 101/101,Loss: 0.130932, RMSE: 0.399925: 100%|██████████| 101/101 [00:55<00:00,  1.83it/s]\n",
            "Val Batch 10/10,Loss: 0.230451, RMSE: 0.396431: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.396431, Val Loss: 0.160936\n",
            "Train RMSE: 0.399925, Average Train Loss: 0.159664, Val RMSE: 0.396431, Average Valid Loss: 0.160936\n",
            "\n",
            "Epoch 52/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 52, Batch 101/101,Loss: 0.072555, RMSE: 0.401081: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.191205, RMSE: 0.377790: 100%|██████████| 10/10 [00:04<00:00,  2.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.377790, Val Loss: 0.145225\n",
            "Train RMSE: 0.401081, Average Train Loss: 0.160025, Val RMSE: 0.377790, Average Valid Loss: 0.145225\n",
            "\n",
            "Epoch 53/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 53, Batch 101/101,Loss: 0.126664, RMSE: 0.398691: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.316937, RMSE: 0.435534: 100%|██████████| 10/10 [00:04<00:00,  2.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.435534, Val Loss: 0.196251\n",
            "Train RMSE: 0.398691, Average Train Loss: 0.158647, Val RMSE: 0.435534, Average Valid Loss: 0.196251\n",
            "\n",
            "Epoch 54/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 54, Batch 101/101,Loss: 0.091607, RMSE: 0.399099: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.211649, RMSE: 0.393407: 100%|██████████| 10/10 [00:04<00:00,  2.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.393407, Val Loss: 0.157702\n",
            "Train RMSE: 0.399099, Average Train Loss: 0.158636, Val RMSE: 0.393407, Average Valid Loss: 0.157702\n",
            "\n",
            "Epoch 55/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 55, Batch 101/101,Loss: 0.188939, RMSE: 0.400125: 100%|██████████| 101/101 [00:57<00:00,  1.76it/s]\n",
            "Val Batch 10/10,Loss: 0.226677, RMSE: 0.396587: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.396587, Val Loss: 0.160859\n",
            "Train RMSE: 0.400125, Average Train Loss: 0.160374, Val RMSE: 0.396587, Average Valid Loss: 0.160859\n",
            "\n",
            "Epoch 56/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 56, Batch 101/101,Loss: 0.102022, RMSE: 0.397662: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.304902, RMSE: 0.436116: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.436116, Val Loss: 0.196112\n",
            "Train RMSE: 0.397662, Average Train Loss: 0.157601, Val RMSE: 0.436116, Average Valid Loss: 0.196112\n",
            "\n",
            "Epoch 57/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 57, Batch 101/101,Loss: 0.080622, RMSE: 0.396163: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.195988, RMSE: 0.391765: 100%|██████████| 10/10 [00:04<00:00,  2.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.391765, Val Loss: 0.155672\n",
            "Train RMSE: 0.396163, Average Train Loss: 0.156219, Val RMSE: 0.391765, Average Valid Loss: 0.155672\n",
            "\n",
            "Epoch 58/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 58, Batch 101/101,Loss: 0.208809, RMSE: 0.396675: 100%|██████████| 101/101 [00:55<00:00,  1.83it/s]\n",
            "Val Batch 10/10,Loss: 0.176019, RMSE: 0.406527: 100%|██████████| 10/10 [00:03<00:00,  2.52it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.406527, Val Loss: 0.165819\n",
            "Train RMSE: 0.396675, Average Train Loss: 0.157841, Val RMSE: 0.406527, Average Valid Loss: 0.165819\n",
            "\n",
            "Epoch 59/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 59, Batch 101/101,Loss: 0.191971, RMSE: 0.396191: 100%|██████████| 101/101 [00:55<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.219524, RMSE: 0.414962: 100%|██████████| 10/10 [00:04<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.414962, Val Loss: 0.174634\n",
            "Train RMSE: 0.396191, Average Train Loss: 0.157300, Val RMSE: 0.414962, Average Valid Loss: 0.174634\n",
            "\n",
            "Epoch 60/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 60, Batch 101/101,Loss: 0.124531, RMSE: 0.396778: 100%|██████████| 101/101 [00:55<00:00,  1.83it/s]\n",
            "Val Batch 10/10,Loss: 0.284849, RMSE: 0.431199: 100%|██████████| 10/10 [00:04<00:00,  2.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.431199, Val Loss: 0.191033\n",
            "Train RMSE: 0.396778, Average Train Loss: 0.157120, Val RMSE: 0.431199, Average Valid Loss: 0.191033\n",
            "\n",
            "Epoch 61/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 61, Batch 101/101,Loss: 0.097632, RMSE: 0.396647: 100%|██████████| 101/101 [00:56<00:00,  1.78it/s]\n",
            "Val Batch 10/10,Loss: 0.206450, RMSE: 0.395542: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.395542, Val Loss: 0.159032\n",
            "Train RMSE: 0.396647, Average Train Loss: 0.156761, Val RMSE: 0.395542, Average Valid Loss: 0.159032\n",
            "\n",
            "Epoch 62/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 62, Batch 101/101,Loss: 0.033207, RMSE: 0.394931: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.176978, RMSE: 0.391315: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.391315, Val Loss: 0.154357\n",
            "Train RMSE: 0.394931, Average Train Loss: 0.154803, Val RMSE: 0.391315, Average Valid Loss: 0.154357\n",
            "\n",
            "Epoch 63/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 63, Batch 101/101,Loss: 0.274138, RMSE: 0.394670: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.251629, RMSE: 0.422771: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.422771, Val Loss: 0.182494\n",
            "Train RMSE: 0.394670, Average Train Loss: 0.156891, Val RMSE: 0.422771, Average Valid Loss: 0.182494\n",
            "\n",
            "Epoch 64/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 64, Batch 101/101,Loss: 0.031601, RMSE: 0.393857: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.169198, RMSE: 0.389829: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.389829, Val Loss: 0.152855\n",
            "Train RMSE: 0.393857, Average Train Loss: 0.153948, Val RMSE: 0.389829, Average Valid Loss: 0.152855\n",
            "\n",
            "Epoch 65/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 65, Batch 101/101,Loss: 0.061762, RMSE: 0.393026: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.271142, RMSE: 0.418636: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.418636, Val Loss: 0.180201\n",
            "Train RMSE: 0.393026, Average Train Loss: 0.153588, Val RMSE: 0.418636, Average Valid Loss: 0.180201\n",
            "\n",
            "Epoch 66/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 66, Batch 101/101,Loss: 0.380559, RMSE: 0.393028: 100%|██████████| 101/101 [00:57<00:00,  1.76it/s]\n",
            "Val Batch 10/10,Loss: 0.195547, RMSE: 0.404703: 100%|██████████| 10/10 [00:04<00:00,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.404703, Val Loss: 0.165423\n",
            "Train RMSE: 0.393028, Average Train Loss: 0.156622, Val RMSE: 0.404703, Average Valid Loss: 0.165423\n",
            "\n",
            "Epoch 67/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 67, Batch 101/101,Loss: 0.056741, RMSE: 0.393112: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.168771, RMSE: 0.388891: 100%|██████████| 10/10 [00:03<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.388891, Val Loss: 0.152140\n",
            "Train RMSE: 0.393112, Average Train Loss: 0.153607, Val RMSE: 0.388891, Average Valid Loss: 0.152140\n",
            "\n",
            "Epoch 68/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 68, Batch 101/101,Loss: 0.087547, RMSE: 0.391942: 100%|██████████| 101/101 [00:56<00:00,  1.78it/s]\n",
            "Val Batch 10/10,Loss: 0.273240, RMSE: 0.424678: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.424678, Val Loss: 0.185141\n",
            "Train RMSE: 0.391942, Average Train Loss: 0.152990, Val RMSE: 0.424678, Average Valid Loss: 0.185141\n",
            "\n",
            "Epoch 69/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 69, Batch 101/101,Loss: 0.454780, RMSE: 0.391972: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.231470, RMSE: 0.401447: 100%|██████████| 10/10 [00:04<00:00,  2.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.401447, Val Loss: 0.164785\n",
            "Train RMSE: 0.391972, Average Train Loss: 0.156507, Val RMSE: 0.401447, Average Valid Loss: 0.164785\n",
            "\n",
            "Epoch 70/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 70, Batch 101/101,Loss: 0.149365, RMSE: 0.391313: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.267865, RMSE: 0.428347: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.428347, Val Loss: 0.187832\n",
            "Train RMSE: 0.391313, Average Train Loss: 0.153090, Val RMSE: 0.428347, Average Valid Loss: 0.187832\n",
            "\n",
            "Epoch 71/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 71, Batch 101/101,Loss: 0.256177, RMSE: 0.392125: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.171135, RMSE: 0.387168: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.387168, Val Loss: 0.150994\n",
            "Train RMSE: 0.392125, Average Train Loss: 0.154736, Val RMSE: 0.387168, Average Valid Loss: 0.150994\n",
            "\n",
            "Epoch 72/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 72, Batch 101/101,Loss: 0.272833, RMSE: 0.390611: 100%|██████████| 101/101 [00:56<00:00,  1.78it/s]\n",
            "Val Batch 10/10,Loss: 0.162235, RMSE: 0.398511: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.398511, Val Loss: 0.158988\n",
            "Train RMSE: 0.390611, Average Train Loss: 0.153721, Val RMSE: 0.398511, Average Valid Loss: 0.158988\n",
            "\n",
            "Epoch 73/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 73, Batch 101/101,Loss: 0.192054, RMSE: 0.392116: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.247733, RMSE: 0.410906: 100%|██████████| 10/10 [00:04<00:00,  2.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.410906, Val Loss: 0.172911\n",
            "Train RMSE: 0.392116, Average Train Loss: 0.154120, Val RMSE: 0.410906, Average Valid Loss: 0.172911\n",
            "\n",
            "Epoch 74/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 74, Batch 101/101,Loss: 0.076775, RMSE: 0.389840: 100%|██████████| 101/101 [00:56<00:00,  1.78it/s]\n",
            "Val Batch 10/10,Loss: 0.255465, RMSE: 0.417569: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.417569, Val Loss: 0.178545\n",
            "Train RMSE: 0.389840, Average Train Loss: 0.151260, Val RMSE: 0.417569, Average Valid Loss: 0.178545\n",
            "\n",
            "Epoch 75/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 75, Batch 101/101,Loss: 0.225343, RMSE: 0.390248: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.269468, RMSE: 0.419948: 100%|██████████| 10/10 [00:04<00:00,  2.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.419948, Val Loss: 0.181158\n",
            "Train RMSE: 0.390248, Average Train Loss: 0.152988, Val RMSE: 0.419948, Average Valid Loss: 0.181158\n",
            "\n",
            "Epoch 76/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 76, Batch 101/101,Loss: 0.281701, RMSE: 0.389601: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.227541, RMSE: 0.410247: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.410247, Val Loss: 0.171357\n",
            "Train RMSE: 0.389601, Average Train Loss: 0.153025, Val RMSE: 0.410247, Average Valid Loss: 0.171357\n",
            "\n",
            "Epoch 77/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 77, Batch 101/101,Loss: 0.125580, RMSE: 0.390788: 100%|██████████| 101/101 [00:58<00:00,  1.74it/s]\n",
            "Val Batch 10/10,Loss: 0.278047, RMSE: 0.431856: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.431856, Val Loss: 0.191220\n",
            "Train RMSE: 0.390788, Average Train Loss: 0.152457, Val RMSE: 0.431856, Average Valid Loss: 0.191220\n",
            "\n",
            "Epoch 78/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 78, Batch 101/101,Loss: 0.188754, RMSE: 0.389185: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.161752, RMSE: 0.402932: 100%|██████████| 10/10 [00:04<00:00,  2.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.402932, Val Loss: 0.162323\n",
            "Train RMSE: 0.389185, Average Train Loss: 0.151820, Val RMSE: 0.402932, Average Valid Loss: 0.162323\n",
            "\n",
            "Epoch 79/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 79, Batch 101/101,Loss: 0.096406, RMSE: 0.390406: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.268146, RMSE: 0.426968: 100%|██████████| 10/10 [00:04<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.426968, Val Loss: 0.186728\n",
            "Train RMSE: 0.390406, Average Train Loss: 0.151884, Val RMSE: 0.426968, Average Valid Loss: 0.186728\n",
            "\n",
            "Epoch 80/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 80, Batch 101/101,Loss: 0.053354, RMSE: 0.388721: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.161950, RMSE: 0.411668: 100%|██████████| 10/10 [00:04<00:00,  2.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.411668, Val Loss: 0.169083\n",
            "Train RMSE: 0.388721, Average Train Loss: 0.150174, Val RMSE: 0.411668, Average Valid Loss: 0.169083\n",
            "\n",
            "Epoch 81/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 81, Batch 101/101,Loss: 0.080630, RMSE: 0.388013: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.271506, RMSE: 0.422878: 100%|██████████| 10/10 [00:04<00:00,  2.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.422878, Val Loss: 0.183604\n",
            "Train RMSE: 0.388013, Average Train Loss: 0.149889, Val RMSE: 0.422878, Average Valid Loss: 0.183604\n",
            "\n",
            "Epoch 82/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 82, Batch 101/101,Loss: 0.254798, RMSE: 0.388262: 100%|██████████| 101/101 [00:56<00:00,  1.78it/s]\n",
            "Val Batch 10/10,Loss: 0.177430, RMSE: 0.417062: 100%|██████████| 10/10 [00:04<00:00,  2.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.417062, Val Loss: 0.174120\n",
            "Train RMSE: 0.388262, Average Train Loss: 0.151738, Val RMSE: 0.417062, Average Valid Loss: 0.174120\n",
            "\n",
            "Epoch 83/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 83, Batch 101/101,Loss: 0.133728, RMSE: 0.389080: 100%|██████████| 101/101 [00:56<00:00,  1.78it/s]\n",
            "Val Batch 10/10,Loss: 0.240091, RMSE: 0.426130: 100%|██████████| 10/10 [00:04<00:00,  2.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.426130, Val Loss: 0.184604\n",
            "Train RMSE: 0.389080, Average Train Loss: 0.151215, Val RMSE: 0.426130, Average Valid Loss: 0.184604\n",
            "\n",
            "Epoch 84/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 84, Batch 101/101,Loss: 0.087896, RMSE: 0.387515: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.192607, RMSE: 0.408323: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.408323, Val Loss: 0.168062\n",
            "Train RMSE: 0.387515, Average Train Loss: 0.149576, Val RMSE: 0.408323, Average Valid Loss: 0.168062\n",
            "\n",
            "Epoch 85/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 85, Batch 101/101,Loss: 0.101545, RMSE: 0.388821: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.258313, RMSE: 0.424682: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.424682, Val Loss: 0.184375\n",
            "Train RMSE: 0.388821, Average Train Loss: 0.150709, Val RMSE: 0.424682, Average Valid Loss: 0.184375\n",
            "\n",
            "Epoch 86/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 86, Batch 101/101,Loss: 0.095700, RMSE: 0.387650: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.175613, RMSE: 0.393683: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.393683, Val Loss: 0.156050\n",
            "Train RMSE: 0.387650, Average Train Loss: 0.149753, Val RMSE: 0.393683, Average Valid Loss: 0.156050\n",
            "\n",
            "Epoch 87/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 87, Batch 101/101,Loss: 0.037362, RMSE: 0.389073: 100%|██████████| 101/101 [00:55<00:00,  1.83it/s]\n",
            "Val Batch 10/10,Loss: 0.216987, RMSE: 0.407151: 100%|██████████| 10/10 [00:04<00:00,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.407151, Val Loss: 0.168413\n",
            "Train RMSE: 0.389073, Average Train Loss: 0.150293, Val RMSE: 0.407151, Average Valid Loss: 0.168413\n",
            "\n",
            "Epoch 88/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 88, Batch 101/101,Loss: 0.140477, RMSE: 0.388243: 100%|██████████| 101/101 [00:57<00:00,  1.76it/s]\n",
            "Val Batch 10/10,Loss: 0.175577, RMSE: 0.397708: 100%|██████████| 10/10 [00:04<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.397708, Val Loss: 0.159069\n",
            "Train RMSE: 0.388243, Average Train Loss: 0.150635, Val RMSE: 0.397708, Average Valid Loss: 0.159069\n",
            "\n",
            "Epoch 89/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 89, Batch 101/101,Loss: 0.411254, RMSE: 0.387650: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.202800, RMSE: 0.400675: 100%|██████████| 10/10 [00:04<00:00,  2.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.400675, Val Loss: 0.162719\n",
            "Train RMSE: 0.387650, Average Train Loss: 0.152755, Val RMSE: 0.400675, Average Valid Loss: 0.162719\n",
            "\n",
            "Epoch 90/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 90, Batch 101/101,Loss: 0.113755, RMSE: 0.387115: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.177232, RMSE: 0.398692: 100%|██████████| 10/10 [00:04<00:00,  2.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.398692, Val Loss: 0.159898\n",
            "Train RMSE: 0.387115, Average Train Loss: 0.149514, Val RMSE: 0.398692, Average Valid Loss: 0.159898\n",
            "\n",
            "Epoch 91/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 91, Batch 101/101,Loss: 0.033758, RMSE: 0.387164: 100%|██████████| 101/101 [00:55<00:00,  1.83it/s]\n",
            "Val Batch 10/10,Loss: 0.171922, RMSE: 0.394162: 100%|██████████| 10/10 [00:03<00:00,  2.50it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.394162, Val Loss: 0.156218\n",
            "Train RMSE: 0.387164, Average Train Loss: 0.148791, Val RMSE: 0.394162, Average Valid Loss: 0.156218\n",
            "\n",
            "Epoch 92/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 92, Batch 101/101,Loss: 0.157000, RMSE: 0.387791: 100%|██████████| 101/101 [00:56<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.177936, RMSE: 0.421978: 100%|██████████| 10/10 [00:04<00:00,  2.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.421978, Val Loss: 0.178059\n",
            "Train RMSE: 0.387791, Average Train Loss: 0.150444, Val RMSE: 0.421978, Average Valid Loss: 0.178059\n",
            "\n",
            "Epoch 93/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 93, Batch 101/101,Loss: 0.160111, RMSE: 0.387354: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.184518, RMSE: 0.406839: 100%|██████████| 10/10 [00:05<00:00,  1.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.406839, Val Loss: 0.166497\n",
            "Train RMSE: 0.387354, Average Train Loss: 0.150139, Val RMSE: 0.406839, Average Valid Loss: 0.166497\n",
            "\n",
            "Epoch 94/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 94, Batch 101/101,Loss: 0.118841, RMSE: 0.386198: 100%|██████████| 101/101 [00:55<00:00,  1.80it/s]\n",
            "Val Batch 10/10,Loss: 0.204185, RMSE: 0.408579: 100%|██████████| 10/10 [00:04<00:00,  2.49it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.408579, Val Loss: 0.168857\n",
            "Train RMSE: 0.386198, Average Train Loss: 0.148861, Val RMSE: 0.408579, Average Valid Loss: 0.168857\n",
            "\n",
            "Epoch 95/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 95, Batch 101/101,Loss: 0.029222, RMSE: 0.387414: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.218749, RMSE: 0.416401: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.416401, Val Loss: 0.175729\n",
            "Train RMSE: 0.387414, Average Train Loss: 0.148940, Val RMSE: 0.416401, Average Valid Loss: 0.175729\n",
            "\n",
            "Epoch 96/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 96, Batch 101/101,Loss: 0.105903, RMSE: 0.386352: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.268720, RMSE: 0.427651: 100%|██████████| 10/10 [00:04<00:00,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.427651, Val Loss: 0.187311\n",
            "Train RMSE: 0.386352, Average Train Loss: 0.148855, Val RMSE: 0.427651, Average Valid Loss: 0.187311\n",
            "\n",
            "Epoch 97/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 97, Batch 101/101,Loss: 0.089364, RMSE: 0.387741: 100%|██████████| 101/101 [00:55<00:00,  1.81it/s]\n",
            "Val Batch 10/10,Loss: 0.254157, RMSE: 0.419233: 100%|██████████| 10/10 [00:04<00:00,  2.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.419233, Val Loss: 0.179799\n",
            "Train RMSE: 0.387741, Average Train Loss: 0.149763, Val RMSE: 0.419233, Average Valid Loss: 0.179799\n",
            "\n",
            "Epoch 98/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 98, Batch 101/101,Loss: 0.367050, RMSE: 0.387167: 100%|██████████| 101/101 [00:56<00:00,  1.79it/s]\n",
            "Val Batch 10/10,Loss: 0.178964, RMSE: 0.402174: 100%|██████████| 10/10 [00:04<00:00,  2.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.402174, Val Loss: 0.162632\n",
            "Train RMSE: 0.387167, Average Train Loss: 0.151965, Val RMSE: 0.402174, Average Valid Loss: 0.162632\n",
            "\n",
            "Epoch 99/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 99, Batch 101/101,Loss: 0.082547, RMSE: 0.386822: 100%|██████████| 101/101 [00:57<00:00,  1.76it/s]\n",
            "Val Batch 10/10,Loss: 0.174456, RMSE: 0.413538: 100%|██████████| 10/10 [00:04<00:00,  2.07it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.413538, Val Loss: 0.171191\n",
            "Train RMSE: 0.386822, Average Train Loss: 0.148993, Val RMSE: 0.413538, Average Valid Loss: 0.171191\n",
            "\n",
            "Epoch 100/100: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Epoch 100, Batch 101/101,Loss: 0.427852, RMSE: 0.386252: 100%|██████████| 101/101 [00:55<00:00,  1.82it/s]\n",
            "Val Batch 10/10,Loss: 0.179654, RMSE: 0.390877: 100%|██████████| 10/10 [00:04<00:00,  2.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val RMSE: 0.390877, Val Loss: 0.154171\n",
            "Train RMSE: 0.386252, Average Train Loss: 0.151842, Val RMSE: 0.390877, Average Valid Loss: 0.154171\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model_weights_challenge_1.pt')"
      ],
      "metadata": {
        "id": "e0rFK18Z6_d8"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.state_dict())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp3RIjmTK8lz",
        "outputId": "c41b10f6-247b-4568-8300-c69b53b11f63"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OrderedDict({'block_1.1.weight': tensor([[[[-7.5011e-02,  1.0098e-01, -1.9283e-02,  1.3473e-01,  1.3163e-01,\n",
            "            1.3963e-01,  1.4651e-01,  1.0248e-02, -7.8975e-02, -4.8278e-02,\n",
            "            1.4744e-02,  7.3645e-02, -2.3366e-02,  3.4903e-02,  7.8428e-02,\n",
            "           -1.3044e-02, -4.8524e-02,  3.8590e-02,  1.8508e-02, -7.1009e-03,\n",
            "           -1.0633e-01,  5.7422e-02, -3.1136e-02, -5.1034e-02,  4.0973e-02,\n",
            "           -5.9263e-02,  5.1305e-02, -5.2801e-02,  4.8022e-02,  3.3509e-02,\n",
            "            4.7258e-02, -5.4056e-02,  2.7368e-02, -1.9561e-02,  8.9956e-02,\n",
            "            9.6563e-02, -2.4960e-02,  8.8679e-02, -4.7450e-04, -6.8953e-02,\n",
            "            6.8167e-02,  4.8882e-02,  9.2060e-02, -2.1475e-02,  7.8569e-02,\n",
            "           -1.0743e-01, -9.2860e-02,  3.9631e-03, -3.3669e-02, -2.3431e-02,\n",
            "           -1.0935e-01, -5.2011e-02,  2.1382e-02, -5.9367e-03,  7.0558e-02,\n",
            "           -2.0812e-02,  1.2929e-01,  1.3423e-01,  1.6437e-01,  1.5494e-01,\n",
            "            9.1586e-02, -2.8003e-02, -2.3535e-02,  1.4290e-01]]],\n",
            "\n",
            "\n",
            "        [[[-3.0922e-02,  7.2131e-02,  5.3367e-02, -1.4922e-01,  6.4449e-02,\n",
            "           -1.2167e-01, -1.2661e-02, -9.1709e-02,  3.2519e-02, -6.7666e-03,\n",
            "           -2.1602e-02, -4.1581e-02, -3.1247e-02, -4.5671e-02, -3.8792e-02,\n",
            "           -2.3952e-02, -2.6305e-02, -2.2612e-02,  6.1458e-02, -1.0201e-01,\n",
            "            3.7800e-02, -4.9483e-02, -1.4472e-01, -1.0868e-01, -3.7211e-02,\n",
            "           -1.1757e-01, -4.0594e-02, -6.6037e-02, -3.9940e-02,  6.3812e-02,\n",
            "            1.1856e-02,  1.1738e-01, -1.2489e-02,  1.0258e-01, -4.8797e-02,\n",
            "            1.3247e-02, -7.3667e-02,  7.3324e-02,  7.5200e-02, -5.6300e-02,\n",
            "            1.7150e-01, -1.9723e-02,  8.8827e-02,  1.2090e-01,  9.3077e-02,\n",
            "           -1.1086e-02,  8.2838e-02, -2.3748e-02,  1.0414e-01,  9.5112e-02,\n",
            "           -3.5791e-02, -8.9074e-03, -4.4440e-02,  1.1433e-01, -7.1289e-02,\n",
            "            9.1750e-02,  5.0057e-02,  8.5795e-02,  7.7771e-02,  8.7028e-02,\n",
            "           -7.7365e-02, -6.8152e-02, -8.2872e-02,  6.5689e-02]]],\n",
            "\n",
            "\n",
            "        [[[-7.6862e-02,  1.1840e-02,  4.4451e-02, -1.1251e-01, -4.0347e-02,\n",
            "           -1.4020e-02, -9.8048e-02,  7.6066e-02,  1.0591e-01,  2.5794e-02,\n",
            "            1.2795e-02, -2.7882e-02,  3.5883e-02,  1.0676e-01, -1.3039e-01,\n",
            "            9.7634e-02, -8.2562e-02, -3.7630e-02,  6.0901e-02, -3.3329e-02,\n",
            "            2.7115e-02,  6.2208e-02, -2.8189e-02,  1.0344e-02,  1.0289e-01,\n",
            "            1.4855e-02, -8.3484e-02, -5.7014e-02,  9.0581e-02,  4.1382e-04,\n",
            "            8.1218e-02, -2.5668e-02, -5.5285e-02,  5.9023e-02, -7.6389e-02,\n",
            "           -2.1696e-02,  8.4163e-02, -1.3076e-01, -1.3167e-01, -1.1406e-01,\n",
            "           -4.1571e-02,  9.9812e-03,  2.6137e-02,  2.2453e-02,  7.1987e-02,\n",
            "            1.3040e-01,  1.4130e-01,  1.3786e-02,  5.3644e-02,  1.0046e-01,\n",
            "            9.6716e-02,  4.5929e-02,  5.7724e-02,  5.4140e-02, -9.1037e-02,\n",
            "           -3.9942e-02, -1.0212e-02, -1.1795e-01, -4.1345e-02, -1.6006e-01,\n",
            "           -1.4209e-01, -1.1941e-01, -1.6808e-01, -6.6926e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0683e-01, -1.0998e-01, -1.1840e-01,  6.2775e-03, -1.2933e-01,\n",
            "           -1.1388e-01,  6.7155e-02, -1.8869e-02, -1.4458e-01,  1.9512e-02,\n",
            "           -7.4166e-03, -2.4494e-02, -6.4393e-02,  4.7855e-02,  7.8904e-02,\n",
            "            6.6866e-02,  7.1394e-02,  1.3505e-01,  1.0467e-01, -1.2586e-02,\n",
            "           -1.7889e-02,  1.7561e-01, -7.2361e-02, -1.3104e-04, -1.8705e-02,\n",
            "            1.3750e-01, -5.0757e-02,  1.0272e-01,  8.0950e-02, -2.6272e-02,\n",
            "           -2.5474e-02, -3.9473e-03,  4.7161e-02,  8.4989e-02,  5.6714e-03,\n",
            "           -6.4115e-02, -1.1167e-01, -6.7322e-02, -7.2016e-02, -1.9744e-02,\n",
            "            9.7877e-02, -1.0109e-01,  1.4913e-02, -6.3725e-02,  3.5220e-02,\n",
            "           -9.6416e-02, -5.4771e-02, -1.4879e-01, -2.8867e-02,  3.7878e-02,\n",
            "           -9.6579e-02,  1.7242e-02,  3.7979e-03,  8.9362e-02, -9.3895e-02,\n",
            "            7.3122e-02,  1.4510e-01,  4.0519e-02, -8.5054e-02,  1.5061e-01,\n",
            "           -3.4022e-02,  1.8519e-01,  1.5061e-01,  7.2009e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.4109e-02, -1.2319e-01, -1.1036e-01, -1.2299e-01, -1.1297e-01,\n",
            "           -2.0769e-01, -1.1257e-01,  3.6018e-02, -3.8990e-03, -2.5072e-02,\n",
            "            9.1046e-02,  1.2969e-01,  1.5823e-01,  1.4207e-01,  1.5040e-02,\n",
            "            1.1851e-01,  1.7662e-01,  3.1453e-02,  1.0715e-01,  5.6339e-02,\n",
            "            4.1295e-02, -4.7743e-02, -7.5326e-02, -1.2200e-01,  4.2794e-02,\n",
            "           -9.8507e-02,  6.5885e-02, -7.6637e-02,  1.0742e-01,  6.3797e-03,\n",
            "            1.1534e-01,  2.2979e-02, -6.6468e-02,  6.1833e-03,  3.4725e-02,\n",
            "           -6.9023e-02,  2.3221e-02,  3.2050e-02, -6.3824e-02,  3.4330e-02,\n",
            "           -2.9497e-03,  7.1563e-02, -5.7127e-02, -9.9182e-02,  7.5576e-02,\n",
            "            5.8925e-02,  1.2030e-01, -8.2037e-02,  3.3974e-02, -4.7040e-02,\n",
            "            7.5706e-02, -1.1954e-01,  2.8089e-02, -6.6652e-02,  4.5091e-02,\n",
            "           -2.8584e-02, -1.0554e-01, -5.4959e-02,  1.0216e-01,  7.1577e-03,\n",
            "           -1.1912e-01,  7.3266e-02, -6.6929e-02, -2.5551e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.6767e-02, -2.0102e-02,  1.1194e-01,  2.2159e-02,  7.0575e-02,\n",
            "            7.9586e-02,  9.5841e-02, -9.6737e-02, -1.2899e-01, -1.2140e-01,\n",
            "           -1.5419e-01, -6.1407e-02, -8.9545e-02, -6.3363e-02,  1.2729e-02,\n",
            "           -6.6500e-02, -2.6136e-03, -4.6528e-02, -1.1311e-01, -1.0762e-01,\n",
            "            1.7732e-02, -1.6974e-02,  9.4363e-02, -1.4195e-02, -2.1678e-02,\n",
            "           -4.2781e-02, -1.9177e-02, -2.7813e-02,  3.7725e-02,  1.4781e-01,\n",
            "            1.3232e-01, -6.8583e-03,  1.0466e-02,  5.6478e-02, -5.3074e-02,\n",
            "            1.4086e-01, -3.0795e-03,  1.7442e-01,  1.3299e-01,  1.2447e-01,\n",
            "            1.2379e-01,  8.4032e-02, -5.3508e-02,  3.4161e-02,  6.8307e-02,\n",
            "           -3.5323e-02, -3.8453e-02, -6.5231e-02, -3.0506e-02,  2.2880e-02,\n",
            "           -8.9646e-02,  6.7789e-02, -7.3155e-02, -8.7958e-02, -6.7616e-02,\n",
            "           -9.7037e-02, -8.6551e-02, -1.2950e-01, -1.5042e-01, -2.6534e-02,\n",
            "           -2.2326e-02, -3.7138e-02,  2.8554e-02, -7.1639e-03]]],\n",
            "\n",
            "\n",
            "        [[[-1.0642e-01, -8.2215e-02, -7.9888e-02, -1.1890e-02, -1.0461e-01,\n",
            "            5.3963e-02, -1.4162e-01, -8.0655e-03,  8.8431e-02,  6.7478e-02,\n",
            "           -7.4482e-02,  4.8695e-02,  1.0908e-01, -2.8924e-02,  1.0470e-01,\n",
            "            7.7810e-02,  3.1352e-02, -2.5935e-02,  2.5057e-02,  1.0813e-01,\n",
            "           -3.6582e-02,  6.1625e-02, -1.0390e-01,  9.1962e-02, -5.9365e-02,\n",
            "           -8.4757e-02, -1.1707e-01,  4.8265e-02,  1.0363e-01,  5.0525e-02,\n",
            "            3.2758e-02,  1.2211e-02, -8.8790e-02, -6.3886e-02, -4.6383e-02,\n",
            "            5.4051e-02, -3.2470e-02,  8.7025e-02, -1.1278e-01,  4.3588e-02,\n",
            "           -7.8231e-02, -3.7334e-02,  7.2035e-02, -3.7986e-03,  1.0410e-01,\n",
            "           -1.1324e-01,  1.0313e-01, -6.3054e-02, -3.6546e-02, -5.5548e-02,\n",
            "            6.5539e-02,  1.5474e-01, -2.2453e-02, -6.7846e-02,  8.0095e-02,\n",
            "           -5.0808e-02, -2.8910e-02,  1.4782e-02,  6.7015e-02, -8.6646e-02,\n",
            "           -1.6189e-02, -4.7593e-02, -1.5598e-01, -3.4904e-02]]],\n",
            "\n",
            "\n",
            "        [[[-6.8563e-02, -5.8429e-02, -5.1478e-02,  3.2514e-02, -1.1470e-01,\n",
            "           -6.9826e-03,  1.2238e-01,  6.9613e-02, -6.9355e-02,  1.1365e-01,\n",
            "            1.1483e-02,  1.1385e-01,  2.5620e-02, -8.5164e-02,  7.7192e-02,\n",
            "           -1.0445e-02, -4.2745e-02,  7.3398e-02, -6.9583e-02, -5.2992e-02,\n",
            "           -6.0907e-02,  1.7876e-02, -1.3334e-01,  3.7186e-03,  1.1086e-01,\n",
            "           -1.1905e-02,  8.2519e-02,  1.2140e-01, -2.8436e-02, -3.8954e-02,\n",
            "           -4.8235e-02,  9.5251e-02,  1.0277e-01,  8.5858e-02, -4.5216e-02,\n",
            "           -7.2000e-02, -1.1369e-01, -1.0432e-01, -1.4045e-01, -1.8624e-02,\n",
            "            6.8330e-02, -1.1399e-01, -3.5889e-02,  3.5806e-02,  3.2240e-02,\n",
            "            3.5325e-02, -3.3368e-03, -3.2038e-02,  4.2351e-02,  7.8182e-02,\n",
            "           -7.4049e-02,  5.8331e-02,  5.7695e-02,  5.5826e-02,  4.5658e-02,\n",
            "            6.7392e-02,  1.2158e-02, -5.9042e-02, -7.8539e-02,  5.8062e-02,\n",
            "           -2.5902e-02, -4.3128e-02,  1.0721e-01, -1.2730e-02]]]],\n",
            "       device='cuda:0'), 'block_1.2.weight': tensor([0.9482, 0.9207, 0.9862, 0.9636, 1.0037, 0.9790, 0.9187, 0.9228],\n",
            "       device='cuda:0'), 'block_1.2.bias': tensor([ 0.0488, -0.0529,  0.0661,  0.0401,  0.0319, -0.0403, -0.0490,  0.0504],\n",
            "       device='cuda:0'), 'block_1.2.running_mean': tensor([ 8.2977e-06, -5.8021e-10, -2.3319e-06,  1.0579e-06, -3.3058e-07,\n",
            "        -4.1178e-06, -1.9478e-06,  5.9515e-07], device='cuda:0'), 'block_1.2.running_var': tensor([5.9369e-06, 8.2801e-06, 3.3548e-06, 4.3579e-06, 4.1781e-06, 7.9799e-06,\n",
            "        1.4294e-06, 7.9316e-07], device='cuda:0'), 'block_1.2.num_batches_tracked': tensor(10100, device='cuda:0'), 'block_2.0.weight': tensor([[[[-1.3007e-02, -3.0024e-02,  3.2437e-02,  ..., -4.1105e-02,\n",
            "           -7.8989e-02, -9.3436e-02]],\n",
            "\n",
            "         [[-5.2731e-02, -5.4592e-02,  4.0501e-02,  ...,  6.5715e-02,\n",
            "            4.9849e-02,  6.6476e-02]],\n",
            "\n",
            "         [[ 1.0468e-01,  6.7578e-02,  6.3244e-02,  ..., -5.0574e-02,\n",
            "           -6.5221e-02, -3.4917e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.8053e-02, -3.0830e-02, -7.6985e-03,  ..., -6.8122e-03,\n",
            "           -1.7025e-02,  1.3876e-02]],\n",
            "\n",
            "         [[ 7.6940e-03, -3.6691e-02, -7.1839e-03,  ...,  5.9396e-02,\n",
            "            3.8625e-02,  8.5705e-02]],\n",
            "\n",
            "         [[ 6.3381e-02,  2.3600e-02,  4.3332e-02,  ..., -3.1493e-02,\n",
            "           -7.2129e-02, -4.8443e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 5.3905e-02, -1.7962e-02, -7.1956e-03,  ...,  9.5889e-02,\n",
            "            8.3874e-02,  7.1365e-03]],\n",
            "\n",
            "         [[ 6.0989e-02,  7.8262e-02,  7.9512e-02,  ...,  2.7893e-02,\n",
            "           -3.5236e-02,  1.3025e-02]],\n",
            "\n",
            "         [[-1.0853e-01, -9.1366e-02, -1.0206e-01,  ..., -5.4091e-02,\n",
            "           -2.6224e-03, -5.0954e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-3.7864e-02,  3.5740e-04, -4.3296e-02,  ..., -3.0158e-02,\n",
            "           -3.7019e-02, -1.4290e-02]],\n",
            "\n",
            "         [[ 2.9570e-02,  6.1189e-02, -9.5365e-03,  ..., -3.3788e-02,\n",
            "           -1.8730e-02, -2.0992e-03]],\n",
            "\n",
            "         [[-6.4955e-03, -2.8510e-02, -1.1104e-03,  ..., -3.6433e-02,\n",
            "            2.0726e-02,  3.0291e-02]]],\n",
            "\n",
            "\n",
            "        [[[-4.7960e-02,  2.9816e-02, -1.8249e-02,  ..., -8.7762e-02,\n",
            "           -4.3599e-02, -1.0657e-01]],\n",
            "\n",
            "         [[ 9.3267e-02,  8.7502e-02,  5.1288e-02,  ...,  6.8331e-02,\n",
            "            7.7624e-02,  4.9115e-02]],\n",
            "\n",
            "         [[-9.5666e-02, -8.3494e-02, -9.1780e-02,  ...,  6.0526e-02,\n",
            "           -1.2656e-02,  4.1423e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-2.2653e-02, -6.0983e-03, -2.2078e-02,  ...,  3.6576e-02,\n",
            "            6.9588e-02,  9.9878e-02]],\n",
            "\n",
            "         [[ 2.8999e-02,  6.8728e-02,  2.4533e-02,  ...,  8.8436e-02,\n",
            "            9.7512e-02,  1.0892e-01]],\n",
            "\n",
            "         [[-4.7425e-02, -3.0420e-02, -1.1628e-03,  ..., -5.6103e-03,\n",
            "           -5.4141e-02, -6.2480e-02]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-6.5666e-02, -8.7101e-02, -4.0443e-02,  ..., -3.3041e-02,\n",
            "           -3.0983e-02, -1.1421e-02]],\n",
            "\n",
            "         [[ 2.2102e-02, -1.1625e-02,  6.2415e-05,  ...,  1.3292e-01,\n",
            "            4.9099e-02,  9.0564e-02]],\n",
            "\n",
            "         [[ 1.8397e-02,  5.8105e-02,  4.5071e-02,  ..., -6.9069e-02,\n",
            "           -3.3784e-02, -1.8067e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-1.7326e-02,  6.0289e-02,  3.1078e-02,  ..., -2.8540e-02,\n",
            "            5.7464e-02,  1.2404e-03]],\n",
            "\n",
            "         [[ 2.3436e-02,  4.5981e-02,  2.8391e-02,  ...,  2.7214e-02,\n",
            "            3.4539e-02, -3.5272e-03]],\n",
            "\n",
            "         [[-6.5775e-03, -4.8361e-02, -1.1973e-02,  ..., -4.5942e-02,\n",
            "           -3.4378e-02,  1.0114e-02]]],\n",
            "\n",
            "\n",
            "        [[[ 4.1701e-02,  5.0781e-02, -2.1251e-02,  ...,  3.3635e-02,\n",
            "            2.0497e-02,  7.3399e-02]],\n",
            "\n",
            "         [[ 1.5848e-02, -2.1297e-02, -6.0559e-02,  ..., -2.8146e-02,\n",
            "           -2.2789e-02,  2.5215e-02]],\n",
            "\n",
            "         [[ 2.4161e-02,  9.9852e-03,  1.5867e-02,  ..., -1.3938e-01,\n",
            "           -8.4984e-02, -1.3107e-01]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 3.7481e-02,  7.2196e-03,  2.2204e-02,  ..., -7.4084e-02,\n",
            "           -8.1297e-02, -1.0712e-01]],\n",
            "\n",
            "         [[-5.4549e-02, -5.5091e-02, -6.0578e-03,  ..., -7.5083e-02,\n",
            "           -4.0513e-02, -5.5930e-02]],\n",
            "\n",
            "         [[-2.3460e-02, -4.0589e-03, -4.6433e-02,  ...,  2.0709e-02,\n",
            "            4.2645e-02,  8.2256e-02]]],\n",
            "\n",
            "\n",
            "        [[[-2.4988e-03, -1.0270e-02,  3.2870e-02,  ...,  9.5730e-02,\n",
            "            9.6100e-02,  5.6308e-02]],\n",
            "\n",
            "         [[ 7.4596e-03, -2.4336e-02,  3.4315e-02,  ..., -6.0552e-03,\n",
            "           -2.9161e-02, -3.7438e-02]],\n",
            "\n",
            "         [[-5.6416e-02, -6.9459e-02, -8.4709e-02,  ...,  2.8838e-02,\n",
            "            1.9761e-02, -2.0128e-02]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 2.4288e-02,  5.6409e-03,  1.4408e-02,  ..., -1.0814e-02,\n",
            "           -2.5360e-02,  1.7982e-02]],\n",
            "\n",
            "         [[ 3.7248e-02,  2.9615e-02,  5.8203e-02,  ...,  1.2364e-04,\n",
            "           -1.1079e-01, -9.9463e-02]],\n",
            "\n",
            "         [[-4.8290e-02, -3.8865e-02, -7.2137e-02,  ..., -6.1448e-02,\n",
            "           -4.0999e-02,  2.1461e-02]]]], device='cuda:0'), 'block_2.1.weight': tensor([1.0364, 0.9696, 0.9661, 0.9565, 0.9438, 0.9415, 0.9363, 0.9807, 1.0077,\n",
            "        0.8761, 0.9962, 0.9696, 0.9925, 0.9694, 0.9636, 0.9780, 0.9137, 0.9333,\n",
            "        1.1683, 1.0020, 0.9765, 1.0316, 0.9976, 1.0134, 0.8617, 0.9635, 0.9280,\n",
            "        0.9663, 0.9392, 0.9673, 1.0848, 0.9927], device='cuda:0'), 'block_2.1.bias': tensor([-0.0033, -0.0100,  0.0011,  0.0195,  0.0174, -0.0094,  0.0123,  0.0211,\n",
            "        -0.0152,  0.0097, -0.0018,  0.0028,  0.0023,  0.0124, -0.0070,  0.0073,\n",
            "        -0.0030,  0.0023,  0.0179,  0.0004,  0.0007,  0.0089, -0.0045, -0.0013,\n",
            "         0.0011, -0.0006,  0.0051, -0.0174, -0.0067,  0.0116,  0.0076,  0.0024],\n",
            "       device='cuda:0'), 'block_2.1.running_mean': tensor([-0.0833,  0.0848, -0.2210,  0.1849,  0.0835, -0.0985,  0.1145,  0.1201,\n",
            "        -0.0061,  0.0867, -0.2544, -0.0066, -0.2745,  0.0675, -0.1609,  0.2015,\n",
            "        -0.0598,  0.1299, -0.0834,  0.0803, -0.1366, -0.0585,  0.0903, -0.3948,\n",
            "         0.3899,  0.1968, -0.0743,  0.2136, -0.1482, -0.2489,  0.1233,  0.0014],\n",
            "       device='cuda:0'), 'block_2.1.running_var': tensor([0.8953, 6.8231, 2.7502, 4.5480, 4.5310, 5.8124, 2.7522, 2.1232, 2.4504,\n",
            "        4.0036, 1.2772, 6.7844, 1.0027, 4.8164, 2.5927, 2.7388, 3.7343, 4.4667,\n",
            "        9.0975, 2.7448, 4.2423, 3.7914, 3.9308, 0.3889, 4.6665, 1.9778, 3.1167,\n",
            "        3.6617, 5.8954, 4.8367, 7.6174, 2.0652], device='cuda:0'), 'block_2.1.num_batches_tracked': tensor(10100, device='cuda:0'), 'block_3.0.parametrizations.weight.original': tensor([[[[-0.0629],\n",
            "          [-0.0678],\n",
            "          [ 0.0010],\n",
            "          ...,\n",
            "          [-0.0137],\n",
            "          [-0.0981],\n",
            "          [-0.0691]]],\n",
            "\n",
            "\n",
            "        [[[-0.0424],\n",
            "          [-0.0755],\n",
            "          [-0.0748],\n",
            "          ...,\n",
            "          [-0.0060],\n",
            "          [-0.0719],\n",
            "          [-0.0462]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0046],\n",
            "          [-0.1037],\n",
            "          [-0.0380],\n",
            "          ...,\n",
            "          [-0.0052],\n",
            "          [ 0.0736],\n",
            "          [-0.0368]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0032],\n",
            "          [-0.0532],\n",
            "          [ 0.1656],\n",
            "          ...,\n",
            "          [ 0.0019],\n",
            "          [ 0.0633],\n",
            "          [-0.0614]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0213],\n",
            "          [-0.1448],\n",
            "          [-0.1073],\n",
            "          ...,\n",
            "          [-0.0014],\n",
            "          [ 0.0272],\n",
            "          [-0.0482]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0008],\n",
            "          [-0.0086],\n",
            "          [-0.0898],\n",
            "          ...,\n",
            "          [-0.0011],\n",
            "          [ 0.0716],\n",
            "          [-0.0224]]]], device='cuda:0'), 'block_3.1.weight': tensor([1.0254, 1.0380, 1.0860, 1.0394, 1.0616, 0.9812, 0.9954, 0.9838, 0.9796,\n",
            "        1.0782, 0.9666, 1.0205, 0.9567, 0.9866, 1.0664, 0.9802, 1.1519, 1.0277,\n",
            "        0.9840, 1.0120, 1.0190, 1.0172, 1.1239, 1.0807, 0.9744, 0.9609, 1.0117,\n",
            "        1.0281, 1.0446, 1.0204, 1.0466, 1.0123, 1.0334, 0.9872, 0.9782, 0.9869,\n",
            "        1.0172, 1.2519, 0.9677, 1.0120, 1.0268, 1.0253, 1.0351, 1.0497, 1.0912,\n",
            "        1.0308, 1.0057, 1.0479, 0.9984, 0.9882, 1.0027, 0.9837, 1.0341, 0.9534,\n",
            "        1.0951, 1.0496, 1.0091, 1.0037, 1.0088, 0.9656, 1.2167, 1.0967, 1.0699,\n",
            "        1.0710], device='cuda:0'), 'block_3.1.bias': tensor([ 0.0098,  0.0843,  0.0181,  0.0426,  0.0313,  0.0494,  0.0485,  0.0419,\n",
            "        -0.0147, -0.0210,  0.0507,  0.0476,  0.0042,  0.0882, -0.0165,  0.0530,\n",
            "         0.0551,  0.0198,  0.0583,  0.0511,  0.0392,  0.0592,  0.0429,  0.0630,\n",
            "         0.0042,  0.0295,  0.0166,  0.0034,  0.0511,  0.0711,  0.0286,  0.0913,\n",
            "         0.0861,  0.0718,  0.0641,  0.0333,  0.0437,  0.0736,  0.0601,  0.0190,\n",
            "         0.0161, -0.0032,  0.1124,  0.0765,  0.0059,  0.0555,  0.1027,  0.0988,\n",
            "         0.0698,  0.0426,  0.0663, -0.0072,  0.0798,  0.0703,  0.0284,  0.0867,\n",
            "         0.0112, -0.0448, -0.0035, -0.0217, -0.1292, -0.0046, -0.0237, -0.0003],\n",
            "       device='cuda:0'), 'block_3.1.running_mean': tensor([ 7.9421e-03,  1.4239e-02, -1.5322e-03,  5.4711e-03, -7.8039e-03,\n",
            "        -4.5802e-03, -3.2428e-02,  1.9433e-02, -1.2746e-02, -1.2078e-02,\n",
            "        -6.7640e-03,  3.3743e-02,  1.3576e-02,  6.2890e-03, -5.7608e-03,\n",
            "         5.1172e-02, -2.1505e-03,  2.0648e-02, -1.1298e-03,  1.6296e-02,\n",
            "        -2.1045e-04,  6.1244e-04,  2.3877e-03,  2.4063e-05,  8.5751e-03,\n",
            "         4.1404e-03,  6.5562e-03, -1.8595e-02,  2.4585e-02,  3.1436e-02,\n",
            "         1.0603e-02, -1.1629e-03, -2.2319e-03, -4.0519e-03,  3.7989e-03,\n",
            "         1.7250e-03, -9.4606e-04,  2.5007e-03,  2.7166e-04,  4.7409e-04,\n",
            "        -3.2798e-04, -7.2117e-04,  4.6721e-03,  3.2445e-03, -2.2324e-03,\n",
            "        -3.0597e-03,  1.6931e-02,  1.5779e-02,  4.6567e-05, -2.3805e-02,\n",
            "        -3.9598e-04, -4.1301e-03, -3.5235e-02, -2.9188e-03, -1.6245e-02,\n",
            "        -4.0512e-02,  2.8163e-03,  4.5370e-03, -2.2222e-02, -7.2862e-04,\n",
            "        -2.5286e-03, -1.5328e-03, -8.0146e-03, -5.8664e-03], device='cuda:0'), 'block_3.1.running_var': tensor([0.1669, 0.2903, 0.0097, 0.0828, 0.7471, 0.1194, 0.1509, 0.0775, 0.1729,\n",
            "        0.1828, 0.2610, 0.0706, 0.1326, 0.0619, 0.1863, 0.2128, 0.0423, 0.0775,\n",
            "        0.1046, 0.0492, 0.2124, 0.1107, 0.0919, 0.0749, 0.1551, 0.1732, 0.1715,\n",
            "        0.1403, 0.1226, 0.2180, 0.2325, 0.1577, 0.0920, 0.0988, 0.1148, 0.1289,\n",
            "        0.0199, 0.0030, 0.0703, 0.0651, 0.1624, 0.1231, 0.1020, 0.0699, 0.0195,\n",
            "        0.1091, 1.6052, 1.2238, 0.1304, 0.1382, 0.2038, 0.1337, 0.1356, 0.1366,\n",
            "        0.0239, 0.0513, 0.1614, 0.0349, 0.1807, 0.2431, 0.1253, 0.1793, 0.1565,\n",
            "        0.1210], device='cuda:0'), 'block_3.1.num_batches_tracked': tensor(10100, device='cuda:0'), 'block_4.0.weight': tensor([[[[-0.0418,  0.0034,  0.0872,  ..., -0.0423, -0.0347, -0.0351]],\n",
            "\n",
            "         [[ 0.0036, -0.0010,  0.0224,  ..., -0.0662, -0.0931, -0.0637]],\n",
            "\n",
            "         [[ 0.0387, -0.0079, -0.0025,  ..., -0.0435, -0.0248, -0.0488]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0216,  0.0037,  0.0044,  ...,  0.0155, -0.0202, -0.0627]],\n",
            "\n",
            "         [[ 0.0391, -0.0133,  0.0133,  ...,  0.0247,  0.0637,  0.0318]],\n",
            "\n",
            "         [[ 0.0419, -0.0110, -0.0141,  ..., -0.0044,  0.0668,  0.1018]]],\n",
            "\n",
            "\n",
            "        [[[-0.0492,  0.0176, -0.0111,  ...,  0.0086,  0.0537,  0.0232]],\n",
            "\n",
            "         [[-0.0501, -0.0277, -0.0335,  ...,  0.0706,  0.0652,  0.0577]],\n",
            "\n",
            "         [[-0.0200,  0.0125, -0.0328,  ..., -0.0274, -0.0234,  0.0003]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0563, -0.0011, -0.0310,  ..., -0.0659, -0.0269,  0.0117]],\n",
            "\n",
            "         [[ 0.0373,  0.0131, -0.0038,  ..., -0.0961, -0.0713, -0.0491]],\n",
            "\n",
            "         [[-0.0234,  0.0646, -0.0050,  ..., -0.0656, -0.0580, -0.0366]]],\n",
            "\n",
            "\n",
            "        [[[-0.0502, -0.0297, -0.0479,  ..., -0.0049, -0.0588, -0.0321]],\n",
            "\n",
            "         [[-0.0308, -0.0327, -0.0188,  ..., -0.0186, -0.0884, -0.0645]],\n",
            "\n",
            "         [[ 0.0129, -0.0233,  0.0247,  ..., -0.0062, -0.0179, -0.0102]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0244,  0.0109,  0.0265,  ..., -0.0156, -0.0155,  0.0135]],\n",
            "\n",
            "         [[ 0.0590, -0.0241,  0.0296,  ..., -0.0099,  0.0571,  0.0414]],\n",
            "\n",
            "         [[ 0.0639, -0.0495, -0.0211,  ...,  0.0107,  0.0488,  0.0986]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[-0.0099, -0.0195,  0.0605,  ...,  0.0345,  0.0228,  0.0627]],\n",
            "\n",
            "         [[ 0.0166,  0.0323,  0.0778,  ...,  0.0238, -0.0003,  0.0061]],\n",
            "\n",
            "         [[-0.0252,  0.0239,  0.0510,  ..., -0.0177, -0.0492,  0.0460]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0147, -0.0072, -0.0025,  ...,  0.0394,  0.0129, -0.0150]],\n",
            "\n",
            "         [[ 0.0340,  0.0304, -0.0077,  ..., -0.0422,  0.0485,  0.0425]],\n",
            "\n",
            "         [[ 0.0040,  0.0085, -0.0110,  ...,  0.0097,  0.0032, -0.0552]]],\n",
            "\n",
            "\n",
            "        [[[-0.0753, -0.0513, -0.0525,  ...,  0.0144, -0.0155, -0.0030]],\n",
            "\n",
            "         [[-0.0165, -0.0494, -0.0263,  ..., -0.0121, -0.0051,  0.0149]],\n",
            "\n",
            "         [[-0.0013, -0.0184,  0.0099,  ...,  0.0507,  0.0924,  0.0860]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0397, -0.0122,  0.0368,  ...,  0.0017,  0.0211,  0.0155]],\n",
            "\n",
            "         [[ 0.0196,  0.0383, -0.0021,  ..., -0.0040, -0.0040, -0.0454]],\n",
            "\n",
            "         [[ 0.0679,  0.0339,  0.0113,  ...,  0.0059,  0.0009, -0.0122]]],\n",
            "\n",
            "\n",
            "        [[[-0.0282, -0.0163, -0.0478,  ..., -0.0113,  0.0242,  0.0103]],\n",
            "\n",
            "         [[-0.0400, -0.0238,  0.0193,  ...,  0.0432,  0.0018,  0.0232]],\n",
            "\n",
            "         [[-0.0073,  0.0264, -0.0074,  ..., -0.0422, -0.0512, -0.0630]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0127, -0.0531, -0.0337,  ..., -0.0381, -0.0209, -0.0240]],\n",
            "\n",
            "         [[ 0.0160, -0.0016,  0.0424,  ...,  0.0009,  0.0192, -0.0070]],\n",
            "\n",
            "         [[-0.0011,  0.0200, -0.0237,  ...,  0.0161, -0.0038, -0.0326]]]],\n",
            "       device='cuda:0'), 'block_4.1.weight': tensor([0.9954, 0.9688, 0.8787, 1.0049, 0.9708, 0.9820, 0.9444, 0.9780, 1.0681,\n",
            "        0.9000, 0.9714, 0.9772, 0.9541, 1.1110, 1.0819, 0.9164, 0.9381, 1.0040,\n",
            "        1.0174, 0.9451, 0.9279, 1.0016, 1.0624, 0.8819, 1.0030, 0.9817, 1.0356,\n",
            "        0.9788, 0.9613, 1.1666, 0.9403, 1.0357], device='cuda:0'), 'block_4.1.bias': tensor([-0.0925,  0.1228,  0.0690,  0.0577,  0.0234,  0.1019,  0.0703, -0.0121,\n",
            "        -0.1100, -0.0682, -0.0175, -0.0716, -0.0979, -0.0784,  0.0048, -0.1315,\n",
            "         0.0138,  0.1542, -0.0353,  0.0525,  0.0700,  0.1247, -0.1148,  0.0157,\n",
            "        -0.0403, -0.0367, -0.0386,  0.0224,  0.0184, -0.0897,  0.1502,  0.0074],\n",
            "       device='cuda:0'), 'block_4.1.running_mean': tensor([-0.3174, -1.3424,  0.0296, -0.1254,  0.4251, -0.3507, -0.1728, -0.2175,\n",
            "         0.6302,  0.7375,  0.1902,  1.4427,  1.2052,  0.2962, -0.3104,  1.0950,\n",
            "        -0.1564, -0.7108,  0.4504,  0.3677, -0.9732, -0.9711,  0.6235,  0.2332,\n",
            "         1.6291,  0.2053,  0.8776, -0.0981, -0.2242,  0.4726, -1.0920,  1.7533],\n",
            "       device='cuda:0'), 'block_4.1.running_var': tensor([3.2177, 4.3760, 3.3696, 6.0376, 2.9778, 3.3545, 3.4557, 6.0142, 3.5861,\n",
            "        6.7547, 5.0341, 6.8206, 3.5415, 2.6226, 2.6165, 4.2973, 5.1373, 3.3077,\n",
            "        3.4703, 4.5680, 5.1826, 3.9230, 3.4438, 5.0745, 8.3664, 2.9934, 5.7314,\n",
            "        2.5835, 5.1447, 3.2874, 5.1220, 9.6067], device='cuda:0'), 'block_4.1.num_batches_tracked': tensor(10100, device='cuda:0'), 'block_5.0.weight': tensor([[[[ 0.0112,  0.0304,  0.0387,  ...,  0.0818, -0.0092, -0.0012]],\n",
            "\n",
            "         [[-0.1245, -0.0445, -0.0008,  ...,  0.0373,  0.0425, -0.0791]],\n",
            "\n",
            "         [[-0.1038,  0.0164,  0.0159,  ..., -0.0623, -0.0156,  0.0838]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0767, -0.0819,  0.0550,  ...,  0.0037,  0.0364,  0.0029]],\n",
            "\n",
            "         [[-0.0056, -0.0131,  0.0363,  ...,  0.0177, -0.0583,  0.0034]],\n",
            "\n",
            "         [[ 0.0352, -0.0850, -0.0995,  ..., -0.0532, -0.0830,  0.0809]]],\n",
            "\n",
            "\n",
            "        [[[-0.0400, -0.0468,  0.0416,  ...,  0.0291,  0.0545,  0.0749]],\n",
            "\n",
            "         [[ 0.0816, -0.0054,  0.0345,  ...,  0.0240,  0.0337, -0.0038]],\n",
            "\n",
            "         [[ 0.0330, -0.0062,  0.0220,  ...,  0.0159,  0.0089,  0.0531]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0547,  0.0477, -0.0502,  ..., -0.0313,  0.1166, -0.0659]],\n",
            "\n",
            "         [[-0.0373,  0.0110,  0.0029,  ..., -0.0727, -0.0355, -0.0385]],\n",
            "\n",
            "         [[-0.0912,  0.0312,  0.0082,  ..., -0.0778, -0.1360, -0.0259]]],\n",
            "\n",
            "\n",
            "        [[[-0.0313,  0.0473,  0.0228,  ...,  0.0098, -0.0745,  0.0783]],\n",
            "\n",
            "         [[-0.0158, -0.0404,  0.0259,  ...,  0.0658,  0.0891,  0.0359]],\n",
            "\n",
            "         [[ 0.0178,  0.0286,  0.0252,  ..., -0.0369, -0.0788,  0.0495]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.1452,  0.0390,  0.0619,  ...,  0.0558, -0.0371,  0.0742]],\n",
            "\n",
            "         [[-0.0032, -0.0555,  0.0312,  ...,  0.0275,  0.0175, -0.1250]],\n",
            "\n",
            "         [[-0.0336, -0.0238,  0.0055,  ..., -0.1055, -0.1306, -0.1225]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[ 0.0265,  0.0524,  0.0712,  ..., -0.0053,  0.0522,  0.0847]],\n",
            "\n",
            "         [[ 0.0070,  0.0545,  0.1020,  ..., -0.0368, -0.0605, -0.0523]],\n",
            "\n",
            "         [[ 0.0338,  0.0648,  0.0443,  ...,  0.0168, -0.0160,  0.0141]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.2275, -0.0296, -0.1826,  ...,  0.0590,  0.0439,  0.0428]],\n",
            "\n",
            "         [[ 0.0514,  0.0426, -0.0294,  ...,  0.0255, -0.0350, -0.0517]],\n",
            "\n",
            "         [[-0.1046, -0.1273, -0.1082,  ..., -0.0080, -0.0428, -0.0151]]],\n",
            "\n",
            "\n",
            "        [[[ 0.0132, -0.0636,  0.0590,  ...,  0.0884,  0.0558, -0.0166]],\n",
            "\n",
            "         [[ 0.0851,  0.0422,  0.0175,  ...,  0.0445, -0.0040,  0.0702]],\n",
            "\n",
            "         [[-0.0286, -0.0031,  0.0479,  ...,  0.0632,  0.0222, -0.0035]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[-0.0923,  0.0246, -0.0314,  ..., -0.0982,  0.0443, -0.0798]],\n",
            "\n",
            "         [[ 0.0188, -0.0406, -0.0451,  ..., -0.0485, -0.0195, -0.0164]],\n",
            "\n",
            "         [[-0.0589,  0.0065,  0.0178,  ..., -0.0112, -0.0462, -0.0492]]],\n",
            "\n",
            "\n",
            "        [[[-0.0025,  0.0084,  0.0094,  ...,  0.0831,  0.0513, -0.0071]],\n",
            "\n",
            "         [[ 0.0506,  0.0078,  0.0057,  ...,  0.0042, -0.0553,  0.0701]],\n",
            "\n",
            "         [[-0.0553, -0.0518,  0.0213,  ...,  0.0861,  0.0767, -0.0455]],\n",
            "\n",
            "         ...,\n",
            "\n",
            "         [[ 0.0471, -0.0045, -0.0150,  ..., -0.1390, -0.0350, -0.1073]],\n",
            "\n",
            "         [[-0.0391,  0.0262, -0.0437,  ..., -0.0685, -0.0048,  0.0575]],\n",
            "\n",
            "         [[-0.0519, -0.0610, -0.0224,  ..., -0.0172, -0.0188, -0.0621]]]],\n",
            "       device='cuda:0'), 'block_5.1.weight': tensor([1.0288, 1.0218, 0.9792, 0.9769, 0.9758, 0.9441, 1.0304, 0.9313],\n",
            "       device='cuda:0'), 'block_5.1.bias': tensor([ 0.0595, -0.0188,  0.0364, -0.0248,  0.0877, -0.0328,  0.0443, -0.0263],\n",
            "       device='cuda:0'), 'block_5.1.running_mean': tensor([-0.0077, -0.2538, -0.1232,  0.0567, -0.0149,  0.5978, -0.1238, -0.0320],\n",
            "       device='cuda:0'), 'block_5.1.running_var': tensor([ 9.7840,  7.1107, 11.3727,  7.0934,  8.6303,  8.3773,  8.2412,  6.9442],\n",
            "       device='cuda:0'), 'block_5.1.num_batches_tracked': tensor(10100, device='cuda:0'), 'final_layer.bias': tensor([1.2126], device='cuda:0'), 'final_layer.parametrizations.weight.original': tensor([[ 0.0747, -0.1775,  0.0712,  0.0237, -0.1344, -0.1265,  0.0602, -0.0987,\n",
            "         -0.1201, -0.1472,  0.0670,  0.0337, -0.1497, -0.0476,  0.0348,  0.0415,\n",
            "          0.0541,  0.0930,  0.0740, -0.1785, -0.1494,  0.0297,  0.1041, -0.2077,\n",
            "         -0.1863,  0.0736,  0.0249, -0.0545,  0.1116,  0.0913, -0.2191, -0.1634,\n",
            "         -0.1440,  0.0596,  0.0247, -0.1865,  0.1095,  0.0417, -0.1328, -0.0763,\n",
            "          0.0512,  0.0655,  0.0626,  0.0243, -0.0839, -0.1599, -0.1774, -0.1805]],\n",
            "       device='cuda:0')})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jdsbPXbpG8wS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}